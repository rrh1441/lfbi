This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: node-modules
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
core/
  artifactStore.ts
  logger.ts
  objectStore.ts
  queue.ts
  supabaseClient.ts
modules/
  dbPortScan.ts
  dnsTwist.ts
  documentExposure.ts
  endpointDiscovery.ts
  nuclei.ts
  rateLimitScan.ts
  shodan.ts
  spfDmarc.ts
  spiderFoot.ts
  tlsScan.ts
  trufflehog.ts
templates/
  dorks-optimized.txt
  dorks.txt
  nuclei-custom.yaml
  testssl.conf
package.json
tsconfig.json
validate-spiderfoot.ts
worker.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="core/artifactStore.ts">
import { Pool } from 'pg';
import { supabase } from './supabaseClient.js';

export const pool = new Pool({
  connectionString: process.env.DATABASE_URL || process.env.DB_URL
});

export interface ArtifactInput {
  type: string;
  val_text: string;
  severity: 'INFO' | 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL';
  src_url?: string;
  sha256?: string;
  mime?: string;
  meta?: Record<string, any>;
}

export interface Finding {
  artifact_id: number;
  finding_type: string;
  recommendation: string;
  description: string;
}

// Insert artifact into database and return ID
export async function insertArtifact(artifact: ArtifactInput): Promise<number> {
  try {
    const result = await pool.query(
      `INSERT INTO artifacts (type, val_text, severity, src_url, sha256, mime, meta, created_at) 
       VALUES ($1, $2, $3, $4, $5, $6, $7, NOW()) 
       RETURNING id`,
      [
        artifact.type,
        artifact.val_text,
        artifact.severity,
        artifact.src_url || null,
        artifact.sha256 || null,
        artifact.mime || null,
        artifact.meta ? JSON.stringify(artifact.meta) : null
      ]
    );
    
    const artifactId = result.rows[0].id;
    
    // Mirror to Supabase
    await supabase.from('findings').insert({
      artifact_id: artifactId,
      type:        artifact.type,
      severity:    artifact.severity,
      val_text:    artifact.val_text,
      src_url:     artifact.src_url ?? null,
      sha256:      artifact.sha256 ?? null,
      mime:        artifact.mime ?? null,
      meta:        artifact.meta ?? {},
      created_at:  new Date().toISOString(),
    }).throwOnError();
    
    console.log(`[artifactStore] Inserted ${artifact.type} artifact: ${artifact.val_text.slice(0, 60)}...`);
    return artifactId;
  } catch (error) {
    console.error('[artifactStore] Insert artifact error:', error);
    throw error;
  }
}

// Insert finding linked to an artifact
export async function insertFinding(
  artifactId: number, 
  findingType: string, 
  recommendation: string, 
  description: string
): Promise<number> {
  try {
    const result = await pool.query(
      `INSERT INTO findings (artifact_id, finding_type, recommendation, description, created_at) 
       VALUES ($1, $2, $3, $4, NOW()) 
       RETURNING id`,
      [artifactId, findingType, recommendation, description]
    );
    
    // Mirror to Supabase
    await supabase.from('findings_detail').insert({
      artifact_id:   artifactId,
      category:      findingType,
      recommendation,
      description,
      created_at:    new Date().toISOString(),
    }).throwOnError();
    
    console.log(`[artifactStore] Inserted finding ${findingType} for artifact ${artifactId}`);
    return result.rows[0].id;
  } catch (error) {
    console.error('[artifactStore] Insert finding error:', error);
    throw error;
  }
}

// Initialize database tables if they don't exist
export async function initializeDatabase(): Promise<void> {
  try {
    // Create artifacts table
    await pool.query(`
      CREATE TABLE IF NOT EXISTS artifacts (
        id SERIAL PRIMARY KEY,
        type VARCHAR(50) NOT NULL,
        val_text TEXT NOT NULL,
        severity VARCHAR(20) NOT NULL,
        src_url TEXT,
        sha256 VARCHAR(64),
        mime VARCHAR(100),
        meta JSONB,
        created_at TIMESTAMP DEFAULT NOW()
      )
    `);

    // Create findings table
    await pool.query(`
      CREATE TABLE IF NOT EXISTS findings (
        id SERIAL PRIMARY KEY,
        artifact_id INTEGER NOT NULL REFERENCES artifacts(id) ON DELETE CASCADE,
        finding_type VARCHAR(50) NOT NULL,
        recommendation TEXT NOT NULL,
        description TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT NOW()
      )
    `);

    // Create scans_master table for tracking scan status
    await pool.query(`
      CREATE TABLE IF NOT EXISTS scans_master (
        scan_id VARCHAR(255) PRIMARY KEY,
        company_name VARCHAR(255),
        domain VARCHAR(255),
        status VARCHAR(50) NOT NULL DEFAULT 'queued',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        completed_at TIMESTAMP WITH TIME ZONE,
        error_message TEXT,
        total_findings_count INTEGER DEFAULT 0,
        max_severity VARCHAR(20)
      )
    `);

    // Create trigger function for updating updated_at
    await pool.query(`
      CREATE OR REPLACE FUNCTION update_updated_at_column()
      RETURNS TRIGGER AS $$
      BEGIN
         NEW.updated_at = NOW();
         RETURN NEW;
      END;
      $$ language 'plpgsql';
    `);

    // Create trigger for scans_master
    await pool.query(`
      DROP TRIGGER IF EXISTS update_scans_master_updated_at ON scans_master;
      CREATE TRIGGER update_scans_master_updated_at
      BEFORE UPDATE ON scans_master
      FOR EACH ROW
      EXECUTE FUNCTION update_updated_at_column();
    `);

    // Create indexes for performance
    await pool.query(`
      CREATE INDEX IF NOT EXISTS idx_artifacts_type ON artifacts(type);
      CREATE INDEX IF NOT EXISTS idx_artifacts_severity ON artifacts(severity);
      CREATE INDEX IF NOT EXISTS idx_artifacts_created_at ON artifacts(created_at);
      CREATE INDEX IF NOT EXISTS idx_findings_artifact_id ON findings(artifact_id);
      CREATE INDEX IF NOT EXISTS idx_findings_type ON findings(finding_type);
      CREATE INDEX IF NOT EXISTS idx_findings_created_at ON findings(created_at);
      CREATE INDEX IF NOT EXISTS idx_scans_master_updated_at ON scans_master(updated_at);
    `);

    console.log('[artifactStore] Database initialized successfully');
  } catch (error) {
    console.error('[artifactStore] Database initialization error:', error);
    throw error;
  }
}
</file>

<file path="core/logger.ts">
export function log(...args: any[]) {
  const timestamp = new Date().toISOString();
  console.log(`[${timestamp}]`, ...args);
}
</file>

<file path="core/objectStore.ts">
import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import fs from 'fs/promises';
import path from 'path';
import { log } from './logger.js';

// Initialize S3 client
const s3Client = new S3Client({
  region: process.env.AWS_REGION || 'us-east-1',
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID!,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,
  },
});

const BUCKET_NAME = process.env.S3_BUCKET_NAME || 'dealbrief-scanner-artifacts';

/**
 * Upload a file to S3-compatible storage
 * @param localPath Local file path to upload
 * @param key S3 object key
 * @param mimeType MIME type of the file
 * @returns Public URL or signed URL of the uploaded file
 */
export async function uploadFile(localPath: string, key: string, mimeType: string): Promise<string> {
  try {
    // Read the file from local path
    const fileBuffer = await fs.readFile(localPath);
    
    // Upload to S3
    const command = new PutObjectCommand({
      Bucket: BUCKET_NAME,
      Key: key,
      Body: fileBuffer,
      ContentType: mimeType,
      // Set metadata for security tracking
      Metadata: {
        'uploaded-by': 'dealbrief-scanner',
        'upload-timestamp': new Date().toISOString(),
      },
    });

    await s3Client.send(command);
    
    // Return the S3 URL
    const url = `https://${BUCKET_NAME}.s3.${process.env.AWS_REGION || 'us-east-1'}.amazonaws.com/${key}`;
    
    log(`[objectStore] File uploaded successfully: ${key}`);
    return url;
    
  } catch (error) {
    log(`[objectStore] Failed to upload file ${localPath}:`, (error as Error).message);
    
    // For development/testing, return a placeholder URL if S3 is not configured
    if (!process.env.AWS_ACCESS_KEY_ID || !process.env.AWS_SECRET_ACCESS_KEY) {
      log(`[objectStore] S3 not configured, returning placeholder URL for ${key}`);
      return `placeholder://storage/${key}`;
    }
    
    throw error;
  }
}

/**
 * Generate a signed URL for downloading a file from S3
 * @param key S3 object key
 * @param expiresIn Expiration time in seconds (default: 1 hour)
 * @returns Signed URL for downloading the file
 */
export async function getDownloadUrl(key: string, expiresIn: number = 3600): Promise<string> {
  try {
    const command = new GetObjectCommand({
      Bucket: BUCKET_NAME,
      Key: key,
    });

    const signedUrl = await getSignedUrl(s3Client, command, { expiresIn });
    return signedUrl;
    
  } catch (error) {
    log(`[objectStore] Failed to generate download URL for ${key}:`, (error as Error).message);
    throw error;
  }
}

/**
 * Check if S3 is properly configured
 * @returns boolean indicating if S3 is configured
 */
export function isS3Configured(): boolean {
  return !!(process.env.AWS_ACCESS_KEY_ID && 
           process.env.AWS_SECRET_ACCESS_KEY && 
           process.env.S3_BUCKET_NAME);
}
</file>

<file path="core/queue.ts">
import { Redis } from '@upstash/redis';

export interface ScanJob {
  id: string;
  companyName: string;
  domain: string;
  createdAt: string;
}

export interface JobStatus {
  id: string;
  state: 'queued' | 'processing' | 'done' | 'failed';
  updated: number;
  message?: string;
  resultUrl?: string;
  error?: string;
}

export class UpstashQueue {
  redis: Redis;

  constructor(url: string) {
    // Parse the Redis URL to extract token and URL for Upstash
    if (url.includes('@')) {
      // Format: redis://username:token@host:port
      const urlObj = new URL(url);
      const token = urlObj.password;
      const restUrl = `https://${urlObj.hostname}`;
      
      this.redis = new Redis({
        url: restUrl,
        token: token
      });
    } else {
      // Fallback to environment variables
      this.redis = Redis.fromEnv();
    }
  }

  async addJob(id: string, job: any): Promise<void> {
    await this.redis.lpush('scan.jobs', JSON.stringify({ ...job, id }));
    await this.redis.hset(`job:${id}`, {
      state: 'queued',
      updated: Date.now().toString(),
      message: 'Scan queued and waiting for processing'
    });
    console.log('[queue] enqueued', id);
  }

  async getNextJob(): Promise<ScanJob | null> {
    try {
      const jobData = await this.redis.rpop('scan.jobs');
      if (!jobData) {
        return null;
      }
      
      console.log('[queue] Raw job data from Redis:', jobData, 'Type:', typeof jobData);
      
      // Handle different data types from Redis
      let jobString: string;
      if (typeof jobData === 'string') {
        jobString = jobData;
      } else if (typeof jobData === 'object') {
        jobString = JSON.stringify(jobData);
      } else {
        jobString = String(jobData);
      }
      
      console.log('[queue] Job string to parse:', jobString);
      
      // Additional safety check - if it doesn't look like JSON, skip it
      if (!jobString.trim().startsWith('{') && !jobString.trim().startsWith('[')) {
        console.log('[queue] Invalid job data format, skipping:', jobString);
        return null;
      }
      
      const job = JSON.parse(jobString) as ScanJob;
      console.log('[queue] Parsed job:', job);
      return job;
    } catch (error) {
      console.error('[queue] Error in getNextJob:', error);
      console.error('[queue] Failed to parse job data, skipping...');
      return null;
    }
  }

  async updateStatus(id: string, state: JobStatus['state'], message?: string, resultUrl?: string): Promise<void> {
    const statusUpdate: Record<string, string> = {
      state,
      updated: Date.now().toString()
    };

    if (message) statusUpdate.message = message;
    if (resultUrl) statusUpdate.resultUrl = resultUrl;

    await this.redis.hset(`job:${id}`, statusUpdate);
    console.log(`[queue] Updated job ${id} status: ${state}${message ? ` - ${message}` : ''}`);
  }

  async getStatus(id: string): Promise<JobStatus | null> {
    const obj = await this.redis.hgetall(`job:${id}`);
    if (!obj || Object.keys(obj).length === 0) return null;
    return obj as unknown as JobStatus;
  }

  // Legacy methods for backwards compatibility
  async nextJob(blockMs = 5000): Promise<[string, ScanJob] | null> {
    const job = await this.getNextJob();
    if (!job) return null;
    return [job.id, job];
  }

  async setStatus(id: string, state: JobStatus['state'], extra: Record<string, any> = {}) {
    await this.redis.hset(`job:${id}`, {
      state,
      updated: Date.now().toString(),
      ...extra
    });
  }
}
</file>

<file path="core/supabaseClient.ts">
import { createClient, SupabaseClient } from '@supabase/supabase-js';

const url = process.env.SUPABASE_URL;
const key = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!url || !key) throw new Error('SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY must be set');

export const supabase: SupabaseClient = createClient(url, key, {
  auth: { persistSession: false },
});
</file>

<file path="modules/dbPortScan.ts">
/*
 * =============================================================================
 * MODULE: dbPortScan.ts (Refactored v2)
 * =============================================================================
 * This module scans for exposed database services, identifies their versions,
 * and checks for known vulnerabilities and common misconfigurations.
 *
 * Key Improvements from previous version:
 * 1.  **Dependency Validation:** Checks for `nmap` and `nuclei` before running.
 * 2.  **Concurrency Control:** Scans multiple targets in parallel for performance.
 * 3.  **Dynamic Vulnerability Scanning:** Leverages `nuclei` for up-to-date
 * vulnerability and misconfiguration scanning.
 * 4.  **Enhanced Service Detection:** Uses `nmap -sV` for accurate results.
 * 5.  **Expanded Configuration Checks:** The list of nmap scripts has been expanded.
 * 6.  **Progress Tracking:** Logs scan progress for long-running jobs.
 * =============================================================================
 */

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import { XMLParser } from 'fast-xml-parser';
import { insertArtifact, insertFinding } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const exec = promisify(execFile);
const xmlParser = new XMLParser({ ignoreAttributes: false });

// REFACTOR: Concurrency control for scanning multiple targets.
const MAX_CONCURRENT_SCANS = 4;

interface Target {
  host: string;
  port: string;
}

interface JobData {
  domain: string;
  scanId?: string;
  targets?: Target[];
}

const PORT_TO_TECH_MAP: Record<string, string> = {
    '5432': 'PostgreSQL',
    '3306': 'MySQL',
    '1433': 'MSSQL',
    '27017': 'MongoDB',
    '6379': 'Redis',
    '8086': 'InfluxDB',
    '9200': 'Elasticsearch',
    '11211': 'Memcached'
};

/**
 * REFACTOR: Validates that required external tools (nmap, nuclei) are installed.
 */
async function validateDependencies(): Promise<{ nmap: boolean; nuclei: boolean }> {
    log('[dbPortScan] Validating dependencies...');
    const checks = await Promise.allSettled([
        exec('nmap', ['--version']),
        exec('nuclei', ['-version'])
    ]);
    const nmapOk = checks[0].status === 'fulfilled';
    const nucleiOk = checks[1].status === 'fulfilled';

    if (!nmapOk) log('[dbPortScan] [CRITICAL] nmap binary not found. Scans will be severely limited.');
    if (!nucleiOk) log('[dbPortScan] [CRITICAL] nuclei binary not found. Dynamic vulnerability scanning is disabled.');

    return { nmap: nmapOk, nuclei: nucleiOk };
}

function getCloudProvider(host: string): string | null {
  if (host.endsWith('.rds.amazonaws.com')) return 'AWS RDS';
  if (host.endsWith('.postgres.database.azure.com')) return 'Azure SQL';
  if (host.endsWith('.sql.azuresynapse.net')) return 'Azure Synapse';
  if (host.endsWith('.db.ondigitalocean.com')) return 'DigitalOcean Managed DB';
  if (host.endsWith('.cloud.timescale.com')) return 'Timescale Cloud';
  if (host.includes('.gcp.datagrid.g.aivencloud.com')) return 'Aiven (GCP)';
  if (host.endsWith('.neon.tech')) return 'Neon';
  return null;
}

async function runNmapScripts(host: string, port: string, type: string, scanId?: string): Promise<void> {
    const scripts: Record<string, string[]> = {
        'MySQL': ['mysql-info', 'mysql-enum', 'mysql-empty-password', 'mysql-vuln-cve2012-2122'],
        'PostgreSQL': ['pgsql-info', 'pgsql-empty-password'],
        'MongoDB': ['mongodb-info', 'mongodb-databases'],
        'Redis': ['redis-info'],
        'MSSQL': ['ms-sql-info', 'ms-sql-empty-password', 'ms-sql-config'],
        'InfluxDB': ['http-enum', 'http-methods'],
        'Elasticsearch': ['http-enum', 'http-methods'],
        'Memcached': ['memcached-info']
    };
    const relevantScripts = scripts[type] || ['banner', 'version']; // Default handler for unknown types

    log(`[dbPortScan] Running Nmap scripts (${relevantScripts.join(',')}) on ${host}:${port}`);
    try {
        const { stdout } = await exec('nmap', ['-Pn', '-p', port, '--script', relevantScripts.join(','), '-oX', '-', host], { timeout: 120000 });
        const result = xmlParser.parse(stdout);
        const scriptOutputs = result?.nmaprun?.host?.ports?.port?.script;
        
        if (!scriptOutputs) return;
        
        for (const script of Array.isArray(scriptOutputs) ? scriptOutputs : [scriptOutputs]) {
            if (script['@_id'] === 'mysql-empty-password' && script['@_output'].includes("root account has empty password")) {
                const artifactId = await insertArtifact({ type: 'db_auth_weakness', val_text: `MySQL root has empty password on ${host}:${port}`, severity: 'CRITICAL', meta: { scan_id: scanId, scan_module: 'dbPortScan', host, port, script: script['@_id'] } });
                await insertFinding(artifactId, 'WEAK_CREDENTIALS', 'Set a strong password for the MySQL root user immediately.', 'Empty root password on an exposed database instance.');
            }
            if (script['@_id'] === 'mongodb-databases') {
                // Handle both elem array and direct output cases
                const hasDatabaseInfo = script.elem?.some((e: any) => e.key === 'databases') || 
                                       script['@_output']?.includes('databases');
                if (hasDatabaseInfo) {
                    const artifactId = await insertArtifact({ type: 'db_misconfiguration', val_text: `MongoDB databases are listable without authentication on ${host}:${port}`, severity: 'HIGH', meta: { scan_id: scanId, scan_module: 'dbPortScan', host, port, script: script['@_id'], output: script['@_output'] } });
                    await insertFinding(artifactId, 'DATABASE_EXPOSURE', 'Configure MongoDB to require authentication to list databases and perform other operations.', 'Database enumeration possible due to missing authentication.');
                }
            }
            if (script['@_id'] === 'memcached-info' && script['@_output']?.includes('version')) {
                const artifactId = await insertArtifact({ type: 'db_service', val_text: `Memcached service exposed on ${host}:${port}`, severity: 'MEDIUM', meta: { scan_id: scanId, scan_module: 'dbPortScan', host, port, script: script['@_id'], output: script['@_output'] } });
                await insertFinding(artifactId, 'DATABASE_EXPOSURE', 'Secure Memcached by binding to localhost only and configuring SASL authentication.', 'Memcached service exposed without authentication.');
            }
        }
    } catch (error) {
        log(`[dbPortScan] Nmap script scan failed for ${host}:${port}:`, (error as Error).message);
    }
}

async function runNucleiForDb(host: string, port: string, type: string, scanId?: string): Promise<void> {
    const techTag = type.toLowerCase();
    log(`[dbPortScan] Running Nuclei scan on ${host}:${port} for technology: ${techTag}`);

    try {
        // REFACTOR: Using tags is more resilient than specific template paths.
        const { stdout } = await exec('nuclei', [
            '-u', `${host}:${port}`,
            '-json',
            '-silent',
            '-timeout', '5',
            '-retries', '1',
            '-tags', `cve,misconfiguration,default-credentials,${techTag}`
        ], { timeout: 300000 });

        const findings = stdout.trim().split('\n').filter(Boolean);
        for (const line of findings) {
            const vuln = JSON.parse(line);
            const severity = (vuln.info.severity.toUpperCase() as any) || 'INFO';
            const cve = (vuln.info.classification?.['cve-id']?.[0] || '').toUpperCase();

            const artifactId = await insertArtifact({
                type: 'vuln',
                val_text: `${vuln.info.name} on ${host}:${port}`,
                severity,
                src_url: cve ? `https://nvd.nist.gov/vuln/detail/${cve}` : vuln.info.reference?.[0],
                meta: {
                    scan_id: scanId,
                    scan_module: 'dbPortScan:nuclei',
                    template_id: vuln['template-id'],
                    vulnerability: vuln.info,
                    host,
                    port
                }
            });
            await insertFinding(artifactId, 'KNOWN_VULNERABILITY', `Remediate based on Nuclei finding details for ${vuln['template-id']}.`, vuln.info.description);
        }
    } catch (error) {
        if ((error as any).stderr && !(error as any).stderr.includes('no templates were loaded')) {
           log(`[dbPortScan] Nuclei scan failed for ${host}:${port}:`, (error as Error).message);
        }
    }
}

/**
 * REFACTOR: Logic for scanning a single target, designed to be run concurrently.
 */
async function scanTarget(target: Target, totalTargets: number, scanId?: string, findingsCount?: { count: number }): Promise<void> {
    const { host, port } = target;
    if (!findingsCount) {
        log(`[dbPortScan] Warning: findingsCount not provided for ${host}:${port}`);
        return;
    }
    
    log(`[dbPortScan] [${findingsCount.count + 1}/${totalTargets}] Scanning ${host}:${port}...`);

    try {
        const { stdout } = await exec('nmap', ['-sV', '-Pn', '-p', port, host, '-oX', '-'], { timeout: 60000 });
        const result = xmlParser.parse(stdout);
        
        const portInfo = result?.nmaprun?.host?.ports?.port;
        if (portInfo?.state?.['@_state'] !== 'open') {
            return; // Port is closed, no finding.
        }

        const service = portInfo.service;
        const serviceProduct = service?.['@_product'] || PORT_TO_TECH_MAP[port] || 'Unknown';
        const serviceVersion = service?.['@_version'] || 'unknown';
        
        log(`[dbPortScan] [OPEN] ${host}:${port} is running ${serviceProduct} ${serviceVersion}`);
        findingsCount.count++; // Increment directly without alias
        
        const cloudProvider = getCloudProvider(host);
        const artifactId = await insertArtifact({
            type: 'db_service',
            val_text: `${serviceProduct} service exposed on ${host}:${port}`,
            severity: 'HIGH',
            meta: { host, port, service_type: serviceProduct, version: serviceVersion, cloud_provider: cloudProvider, scan_id: scanId, scan_module: 'dbPortScan' }
        });
        
        let recommendation = `Secure ${serviceProduct} by restricting network access. Use a firewall, VPN, or IP allow-listing.`;
        if (cloudProvider) {
            recommendation = `Secure ${serviceProduct} on ${cloudProvider} by reviewing security group/firewall rules and checking IAM policies.`;
        }
        await insertFinding(artifactId, 'DATABASE_EXPOSURE', recommendation, `${serviceProduct} service exposed to the internet.`);
        
        await runNmapScripts(host, port, serviceProduct, scanId);
        await runNucleiForDb(host, port, serviceProduct, scanId);

    } catch (error) {
       log(`[dbPortScan] Error scanning ${host}:${port}:`, (error as Error).message);
    }
}


export async function runDbPortScan(job: JobData): Promise<number> {
  log('[dbPortScan] Starting enhanced database security scan for', job.domain);
  
  const { nmap } = await validateDependencies();
  if (!nmap) {
      log('[dbPortScan] CRITICAL: nmap is not available. Aborting scan.');
      return 0;
  }

  const defaultPorts = Object.keys(PORT_TO_TECH_MAP);
  const targets: Target[] = job.targets?.length ? job.targets : defaultPorts.map(port => ({ host: job.domain, port }));
  
  const findingsCounter = { count: 0 };

  // REFACTOR: Process targets in concurrent chunks for performance.
  for (let i = 0; i < targets.length; i += MAX_CONCURRENT_SCANS) {
      const chunk = targets.slice(i, i + MAX_CONCURRENT_SCANS);
      await Promise.all(
          chunk.map(target => scanTarget(target, targets.length, job.scanId, findingsCounter))
      );
  }

  log('[dbPortScan] Completed database scan, found', findingsCounter.count, 'exposed services');
  await insertArtifact({
    type: 'scan_summary',
    val_text: `Database port scan completed: ${findingsCounter.count} exposed services found`,
    severity: 'INFO',
    meta: {
      scan_id: job.scanId,
      scan_module: 'dbPortScan',
      total_findings: findingsCounter.count,
      targets_scanned: targets.length,
      timestamp: new Date().toISOString()
    }
  });
  
  return findingsCounter.count;
}
</file>

<file path="modules/dnsTwist.ts">
/*
 * =============================================================================
 * MODULE: dnsTwist.ts (Refactored v2)
 * =============================================================================
 * This module uses dnstwist to find typosquatted domains and then performs
 * deeper analysis to identify potentially malicious ones.
 *
 * Key Improvements from previous version:
 * 1.  **Wildcard DNS Detection:** Actively checks for wildcard DNS records.
 * 2.  **Sophisticated Phishing Analysis:** Parses HTML to analyze login forms,
 * form actions, and favicons for reliable phishing detection.
 * 3.  **Dynamic Severity Scoring:** Calculates severity based on a combination
 * of factors for accurate prioritization.
 * 4.  **Dual Protocol Support:** Checks both HTTPS and HTTP for live websites.
 * 5.  **Concurrency & Rate Limiting:** Processes multiple domains in parallel
 * with delays to improve performance and avoid being blocked.
 * =============================================================================
 */

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import axios from 'axios';
import { parse } from 'node-html-parser';
import { insertArtifact, insertFinding } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const exec = promisify(execFile);

// REFACTOR: Added concurrency and rate-limiting controls.
const MAX_CONCURRENT_CHECKS = 5;
const DELAY_BETWEEN_BATCHES_MS = 1000;

// --- Supporting Analysis Functions ---

async function getDnsRecords(domain: string): Promise<{ mx: string[], ns: string[] }> {
    const records: { mx: string[], ns:string[] } = { mx: [], ns: [] };
    try {
        const { stdout: mxOut } = await exec('dig', ['MX', '+short', domain]);
        if (mxOut.trim()) records.mx = mxOut.trim().split('\n');
    } catch (e) { /* No MX records */ }

    try {
        const { stdout: nsOut } = await exec('dig', ['NS', '+short', domain]);
        if (nsOut.trim()) records.ns = nsOut.trim().split('\n');
    } catch (e) { /* No NS records */ }

    return records;
}

async function checkCTLogs(domain: string): Promise<Array<{ issuer_name: string, common_name: string }>> {
    try {
        const { data } = await axios.get(`https://crt.sh/?q=%.${domain}&output=json`, { timeout: 10000 });
        if (Array.isArray(data)) {
            const certs = new Map<string, { issuer_name: string, common_name: string }>();
            data.forEach(cert => certs.set(cert.common_name, { issuer_name: cert.issuer_name, common_name: cert.common_name }));
            return Array.from(certs.values()).slice(0, 5);
        }
    } catch (error) {
        log(`[dnstwist] CT log check failed for ${domain}:`, (error as Error).message);
    }
    return [];
}

async function checkForWildcard(domain: string): Promise<boolean> {
    try {
        const randomSubdomain = `${Math.random().toString(36).substring(2, 12)}.${domain}`;
        const { stdout } = await exec('dig', ['A', '+short', randomSubdomain]);
        return stdout.trim().length > 0;
    } catch (e) {
        // REFACTOR: Enhanced error logging.
        log(`[dnstwist] Wildcard check failed for ${domain}:`, (e as Error).message);
        return false;
    }
}

/**
 * REFACTOR: Quick Win - Implements dual protocol support to find HTTPS-first sites.
 */
async function fetchWithFallback(domain: string): Promise<string | null> {
    for (const protocol of ['https', 'http']) {
        try {
            const { data } = await axios.get(`${protocol}://${domain}`, { 
                timeout: 7000,
                // Important for self-signed certs on phishing sites
                httpsAgent: new (await import('https')).Agent({ rejectUnauthorized: false })
            });
            return data;
        } catch (error) {
            continue; // Try next protocol
        }
    }
    return null;
}

async function analyzeWebPageForPhishing(domain: string, originalDomain: string): Promise<{ score: number; evidence: string[] }> {
    const evidence: string[] = [];
    let score = 0;
    
    const html = await fetchWithFallback(domain);
    if (!html) return { score, evidence };

    try {
        const root = parse(html);
        const passwordInput = root.querySelector('input[type="password"]');
        const emailOrUserInput = root.querySelector('input[type="email"], input[type="text"], input[name*="user"], input[name*="login"]');
        
        if (passwordInput && emailOrUserInput) {
            score += 40;
            evidence.push('Page contains both username/email and password fields.');

            const form = passwordInput.closest('form');
            if (form) {
                const action = form.getAttribute('action');
                if (action && !action.startsWith('/') && !action.includes(domain)) {
                    score += 20;
                    evidence.push(`Form submits credentials to a third-party domain: ${action}`);
                }
            }
        }
        
        const favicon = root.querySelector('link[rel*="icon"]');
        if (favicon?.getAttribute('href')?.includes(originalDomain)) {
            score += 15;
            evidence.push(`Favicon is hotlinked from the original domain to appear legitimate.`);
        }
    } catch (error) {
        log(`[dnstwist] HTML parsing failed for ${domain}:`, (error as Error).message);
    }
    
    return { score, evidence };
}

// --- Main Execution Logic ---

export async function runDnsTwist(job: { domain: string; scanId?: string }): Promise<number> {
  log('[dnstwist] Starting enhanced typosquatting scan for', job.domain);
  
  try {
    const { stdout } = await exec('dnstwist', ['-r', job.domain, '--format', 'json'], { timeout: 120000 });
    const list = (JSON.parse(stdout) as Array<{ domain: string; dns_a?: string[]; dns_aaaa?: string[] }>)
        .filter(entry => (entry.dns_a && entry.dns_a.length > 0) || (entry.dns_aaaa && entry.dns_aaaa.length > 0));

    let totalFindings = 0;

    // REFACTOR: Process entries in concurrent batches with delays.
    for (let i = 0; i < list.length; i += MAX_CONCURRENT_CHECKS) {
        const batch = list.slice(i, i + MAX_CONCURRENT_CHECKS);
        log(`[dnstwist] Processing batch ${i / MAX_CONCURRENT_CHECKS + 1} of ${Math.ceil(list.length / MAX_CONCURRENT_CHECKS)}...`);
        
        await Promise.all(batch.map(async (entry) => {
            totalFindings++;
            const { mx: mxRecords, ns: nsRecords } = await getDnsRecords(entry.domain);
            const ctCerts = await checkCTLogs(entry.domain);
            const hasWildcard = await checkForWildcard(entry.domain);
            const phishingAnalysis = await analyzeWebPageForPhishing(entry.domain, job.domain);

            let score = 10;
            if (mxRecords.length > 0) score += 20;
            if (ctCerts.length > 0) score += 15;
            if (hasWildcard) score += 30;
            score += phishingAnalysis.score;
            
            let severity: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' = 'LOW';
            if (score >= 70) severity = 'CRITICAL';
            else if (score >= 50) severity = 'HIGH';
            else if (score >= 25) severity = 'MEDIUM';

            const artifactId = await insertArtifact({
              type: 'typo_domain',
              val_text: `Potentially malicious typosquatted domain detected: ${entry.domain}`,
              severity,
              meta: { 
                scan_id: job.scanId,
                scan_module: 'dnstwist',
                typosquatted_domain: entry.domain,
                ips: [...(entry.dns_a || []), ...(entry.dns_aaaa || [])],
                mx_records: mxRecords,
                ns_records: nsRecords,
                ct_log_certs: ctCerts,
                has_wildcard_dns: hasWildcard,
                phishing_score: phishingAnalysis.score,
                phishing_evidence: phishingAnalysis.evidence,
                severity_score: score,
              }
            });

            if (severity === 'HIGH' || severity === 'CRITICAL') {
                await insertFinding(
                  artifactId,
                  'PHISHING_SETUP',
                  `Investigate and initiate takedown procedures for the suspected malicious domain ${entry.domain}.`,
                  `Domain shows signs of malicious activity (Phishing Score: ${phishingAnalysis.score}, Wildcard: ${hasWildcard}, MX Active: ${mxRecords.length > 0})`
                );
            }
        }));

        if (i + MAX_CONCURRENT_CHECKS < list.length) {
            await new Promise(resolve => setTimeout(resolve, DELAY_BETWEEN_BATCHES_MS));
        }
    }
    
    log('[dnstwist] Completed scan, found', totalFindings, 'potentially active domains');
    return totalFindings;
    
  } catch (error) {
    if ((error as any).code === 'ENOENT') {
      log('[dnstwist] dnstwist command not found. Please ensure it is installed and in the PATH.');
      await insertArtifact({ type: 'scan_error', val_text: 'dnstwist command not found', severity: 'INFO', meta: { scan_id: job.scanId, scan_module: 'dnstwist' } });
    } else {
      log('[dnstwist] Error during scan:', (error as Error).message);
    }
    return 0;
  }
}
</file>

<file path="modules/documentExposure.ts">
/*
 * =============================================================================
 * MODULE: documentExposure.ts (Security-Hardened Refactor v3)
 * =============================================================================
 * This module replaces crmExposure.ts and fileHunt.ts.
 *
 * CRITICAL SECURITY NOTICE:
 * This module downloads and processes untrusted files from the internet. While
 * this version includes timeout, zip bomb checks, magic byte validation, and
 * basic memory monitoring, it DOES NOT sandbox the file parsing process.
 * A vulnerability in a dependency (pdfjs-dist, mammoth, xlsx) could still lead
 * to Remote Code Execution (RCE) in the worker's context.
 *
 * PRODUCTION DEPLOYMENT RECOMMENDATIONS:
 * 1.  **SANDBOXING (MANDATORY):** The `processFileBuffer` function must be
 * executed in a sandboxed environment (e.g., a separate, short-lived
 * container with no network access, a worker thread with resource limits,
 * or a service like AWS Lambda).
 * 2.  **DEPENDENCY SCANNING:** Regularly scan all dependencies (npm audit, Snyk)
 * for known vulnerabilities. The parsers are the primary attack surface.
 * 3.  **VIRUS SCANNING:** Before processing, scan all downloaded files with an
 * antivirus scanner like ClamAV.
 * =============================================================================
 */

import path from 'node:path';
import fs from 'node:fs/promises';
import crypto from 'node:crypto';
import { createRequire } from 'node:module';
import axios from 'axios';
import { fileTypeFromBuffer } from 'file-type';
import { getDocument, GlobalWorkerOptions } from 'pdfjs-dist';
import luhn from 'luhn';
// import mammoth from 'mammoth';
// import xlsx from 'xlsx';
import yauzl from 'yauzl';
// import { detect as detectLanguage } from 'langdetect';

import { insertArtifact, insertFinding } from '../core/artifactStore.js';
import { uploadFile } from '../core/objectStore.js';
import { log } from '../core/logger.js';

// --- Configuration & Initialization ---

const SERPER_URL = 'https://google.serper.dev/search';
const require = createRequire(import.meta.url);

// REFACTOR: Added more constants for security and performance tuning.
const FILE_PROCESSING_TIMEOUT_MS = 30000;
const MAX_UNCOMPRESSED_ZIP_SIZE_MB = 50;
const MAX_CONTENT_ANALYSIS_BYTES = 250000;
const MAX_WORKER_MEMORY_MB = 512; // Max RSS memory before aborting a task.

try {
    GlobalWorkerOptions.workerSrc = require.resolve('pdfjs-dist/build/pdf.worker.mjs');
} catch (error) {
    log('[docExposure] [ERROR] Could not resolve pdf.worker.mjs. PDF processing will fail.', error);
}

// --- Dork & Platform Management ---
async function getDorks(companyName: string, domain: string): Promise<Map<string, string[]>> {
  const dorksByCat = new Map<string, string[]>();
  try {
    const dorksTemplate = await fs.readFile(
      path.resolve(process.cwd(), 'apps/workers/templates/dorks-optimized.txt'),
      'utf-8'
    );
    let currentCategory = 'default';
    for (const line of dorksTemplate.split('\n')) {
        const trimmedLine = line.trim();
        if (trimmedLine.startsWith('# ---')) {
            currentCategory = trimmedLine.replace('# ---', '').trim().toLowerCase();
        } else if (trimmedLine && !trimmedLine.startsWith('#')) {
            const processedDork = trimmedLine.replace(/COMPANY_NAME/g, `"${companyName}"`).replace(/DOMAIN/g, domain);
            if (!dorksByCat.has(currentCategory)) dorksByCat.set(currentCategory, []);
            dorksByCat.get(currentCategory)!.push(processedDork);
        }
    }
    return dorksByCat;
  } catch (error) {
    log('[docExposure] Error reading dork file, using fallback dorks:', (error as Error).message);
    const fallbackDorks = new Map<string, string[]>();
    fallbackDorks.set('fallback', [ `site:*.hubspot.com "${companyName}"`, `"${companyName}" filetype:pdf` ]);
    return fallbackDorks;
  }
}

function getPlatform(url: string): string {
  const lowerUrl = url.toLowerCase();
  if (lowerUrl.includes('hubspot')) return 'HubSpot';
  if (lowerUrl.includes('salesforce') || lowerUrl.includes('force.com')) return 'Salesforce';
  if (lowerUrl.includes('drive.google.com') || lowerUrl.includes('docs.google.com')) return 'Google Drive';
  if (lowerUrl.includes('sharepoint.com')) return 'SharePoint';
  return 'Unknown Cloud Storage';
}

async function verifyMimeType(buffer: Buffer, reportedMime: string): Promise<{ reported: string; verified: string }> {
  try {
    const fileType = await fileTypeFromBuffer(buffer);
    return { reported: reportedMime, verified: fileType?.mime ?? 'unknown' };
  } catch (error) {
    return { reported: reportedMime, verified: 'verification_failed' };
  }
}

// --- Security-Hardened File Processing ---

// REFACTOR: Added Magic Byte validation map.
const MAGIC_BYTES: { [mime: string]: Buffer } = {
    'application/pdf': Buffer.from([0x25, 0x50, 0x44, 0x46]), // %PDF
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': Buffer.from([0x50, 0x4B, 0x03, 0x04]), // PK..
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': Buffer.from([0x50, 0x4B, 0x03, 0x04]), // PK..
};

/**
 * REFACTOR: Quick Win - Validates file headers against known magic bytes
 * to prevent parsers from processing mismatched file types.
 */
function validateFileHeader(buffer: Buffer, verifiedMime: string): boolean {
    const expectedMagicBytes = MAGIC_BYTES[verifiedMime];
    if (!expectedMagicBytes) return true; // No validation for this MIME type
    const actualMagicBytes = buffer.slice(0, expectedMagicBytes.length);
    return actualMagicBytes.equals(expectedMagicBytes);
}

/**
 * REFACTOR: Quick Win - Checks current memory usage to prevent resource exhaustion
 * before starting a heavy parsing operation.
 */
function checkMemoryUsage(): void {
    const memoryUsage = process.memoryUsage().rss; // Resident Set Size
    if (memoryUsage > MAX_WORKER_MEMORY_MB * 1024 * 1024) {
        throw new Error(`Memory limit exceeded (${Math.round(memoryUsage / 1024 / 1024)}MB)`);
    }
}

async function validateZipBomb(buffer: Buffer): Promise<boolean> {
    return new Promise((resolve, reject) => {
        let totalUncompressedSize = 0;
        yauzl.fromBuffer(buffer, { lazyEntries: true }, (err, zipfile) => {
            if (err || !zipfile) return reject(err || new Error('Invalid zip file'));
            zipfile.readEntry();
            zipfile.on('entry', (entry) => {
                // Treat negative size as exceeding limit to avoid bypass
                if (entry.uncompressedSize < 0 || entry.uncompressedSize > MAX_UNCOMPRESSED_ZIP_SIZE_MB * 1024 * 1024) {
                    zipfile.close();
                    return resolve(false); // Exceeds limit or unknown size
                }
                totalUncompressedSize += entry.uncompressedSize;
                if (totalUncompressedSize > MAX_UNCOMPRESSED_ZIP_SIZE_MB * 1024 * 1024) {
                    zipfile.close();
                    return resolve(false); // Exceeds limit
                }
                zipfile.readEntry();
            });
            zipfile.on('end', () => resolve(true)); // Within limit
            zipfile.on('error', (e) => reject(e));
        });
    });
}

async function processFileBuffer(buffer: Buffer, mime: string): Promise<{
    textContent: string;
    metadata?: Record<string, any>;
}> {
    let textContent = '';
    let metadata: Record<string, any> | undefined;

    switch (mime) {
        case 'application/pdf':
            const uint8Array = new Uint8Array(buffer.buffer, buffer.byteOffset, buffer.byteLength);
            const pdfDocument = await getDocument(uint8Array).promise;
            metadata = (await pdfDocument.getMetadata()).info;
            let fullText = '';
            for (let i = 1; i <= pdfDocument.numPages; i++) {
                const page = await pdfDocument.getPage(i);
                const content = await page.getTextContent();
                fullText += content.items.map((item: any) => item.str).join(' ') + '\n';
            }
            textContent = fullText;
            break;
        case 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
            if (!(await validateZipBomb(buffer))) throw new Error('Zip Bomb detected in DOCX');
            // const docxResult = await mammoth.extractRawText({ buffer });
            // textContent = docxResult.value;
            textContent = 'DOCX processing temporarily disabled';
            break;
        case 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet':
            if (!(await validateZipBomb(buffer))) throw new Error('Zip Bomb detected in XLSX');
            // const workbook = xlsx.read(buffer, { type: 'buffer' });
            // textContent = workbook.SheetNames.map((sheetName: string) =>
            //     xlsx.utils.sheet_to_csv(workbook.Sheets[sheetName])
            // ).join('\n');
            textContent = 'XLSX processing temporarily disabled';
            break;
        default:
            textContent = buffer.toString('utf8', 0, MAX_CONTENT_ANALYSIS_BYTES);
            break;
    }
    return { textContent, metadata };
}

async function downloadAndAnalyze(url: string, companyName: string, scanId?: string): Promise<{
  sha256: string;
  mimeInfo: { reported: string; verified: string };
  localPath: string;
  sensitivity: number;
  findings: string[];
  fileMetadata?: Record<string, any>;
  language: string;
} | null> {
  let tmpPath: string | null = null;
  try {
    const headRes = await axios.head(url, { timeout: 10000 }).catch(() => null);
    if (headRes?.headers['content-length'] && parseInt(headRes.headers['content-length'], 10) > 15 * 1024 * 1024) {
      log('[docExposure] File size > 15MB, skipping. URL:', url);
      return null;
    }

    const res = await axios.get<ArrayBuffer>(url, { responseType: 'arraybuffer', timeout: 30000 });
    const buf = Buffer.from(res.data);
    
    // --- Pre-processing Security Checks ---
    const mimeInfo = await verifyMimeType(buf, res.headers['content-type'] ?? 'application/octet-stream');
    if (!validateFileHeader(buf, mimeInfo.verified)) {
        throw new Error(`Magic byte validation failed for ${mimeInfo.verified}`);
    }
    checkMemoryUsage(); // Check memory before heavy lifting

    // --- Secure Processing Stage ---
    const sha256 = crypto.createHash('sha256').update(buf).digest('hex');
    const ext = path.extname(url).split('?')[0].replace(/[^a-z0-9.]/gi, '') || '.tmp';
    tmpPath = `/tmp/doc_${sha256}${ext}`;
    await fs.writeFile(tmpPath, buf);

    const timeoutPromise = new Promise<never>((_, reject) =>
        setTimeout(() => reject(new Error(`File processing timed out after ${FILE_PROCESSING_TIMEOUT_MS}ms`)), FILE_PROCESSING_TIMEOUT_MS)
    );

    const { textContent, metadata } = await Promise.race([
        processFileBuffer(buf, mimeInfo.verified),
        timeoutPromise
    ]);

    const { sensitivity, findings } = analyzeSensitivity(textContent, companyName, metadata);
    // const langResult = detectLanguage(textContent.substring(0, 10000));
    // const language = (langResult?.length > 0) ? langResult[0].lang : 'unknown';
    const language = 'unknown'; // Language detection temporarily disabled

    return { sha256, mimeInfo, localPath: tmpPath, sensitivity, findings, fileMetadata: metadata, language };

  } catch (err) {
    log(`[docExposure] [ERROR] Failed to process ${url}:`, (err as Error).message);
    if ((err as Error).message.includes('Zip Bomb')) {
        await insertArtifact({
            type: 'scan_artefact_error',
            val_text: `Potential Zip Bomb detected and blocked: ${url}`,
            severity: 'MEDIUM',
            meta: { scan_id: scanId, scan_module: 'documentExposure', error: 'Zip Bomb' }
        });
    }
    return null;
  } finally {
      if (tmpPath) await fs.unlink(tmpPath).catch(e => log(`[docExposure] Failed to clean up temp file ${tmpPath}`, e));
  }
}

// --- Sensitivity Scoring (More specific regex) ---
function analyzeSensitivity(content: string, companyName: string, metadata?: Record<string, any>): { sensitivity: number; findings: string[] } {
    const findings: string[] = [];
    let score = 0;
    const lowerContent = content.toLowerCase();

    // Regexes for PII
    const emailRegex = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g;
    const phoneRegex = /(?:\+?1\s*(?:[.-]\s*)?)?(?:\(\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\s*\)|([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\s*(?:[.-]\s*)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\s*(?:[.-]\s*)?([0-9]{4})(?:\s*(?:#|x\.?|ext\.?|extension)\s*(\d+))?/g;

    // Fix credit card validation - iterate over each candidate individually
    const creditCardCandidates = content.match(/\b(?:\d[ -]*?){13,16}\b/g) || [];
    let validCreditCardFound = false;
    for (const candidate of creditCardCandidates) {
        const cleanedNumber = candidate.replace(/\D/g, '');
        if (cleanedNumber.length >= 13 && cleanedNumber.length <= 19 && luhn.validate(cleanedNumber)) {
            validCreditCardFound = true;
            break;
        }
    }
    if (validCreditCardFound) {
        score += 25;
        findings.push('Potential credit card number(s) found');
    }

    if ((content.match(emailRegex) || []).length > 5) {
        score += 10; findings.push('Multiple email addresses found');
    }
    if ((content.match(phoneRegex) || []).length > 0) {
        score += 5; findings.push('Phone number(s) found');
    }

    const confidentialKeywords = ['confidential', 'proprietary', 'internal use only', 'restricted'];
    if (confidentialKeywords.some(k => lowerContent.includes(k))) {
        score += 10; findings.push('Confidential markings found');
    }

    const highEntropyRegex = /[A-Za-z0-9+/]{40,}[=]{0,2}/g;
    if (highEntropyRegex.test(content)) {
        score += 15; findings.push('Potential API keys/tokens found');
    }
    return { sensitivity: score, findings };
}

function getSeverity(score: number): 'INFO' | 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' {
  if (score >= 40) return 'CRITICAL';
  if (score >= 25) return 'HIGH';
  if (score >= 15) return 'MEDIUM';
  if (score > 0) return 'LOW';
  return 'INFO';
}

// --- Main Execution Logic ---
export async function runDocumentExposure(job: { companyName: string; domain: string; scanId?: string }): Promise<number> {
  const { companyName, domain, scanId } = job;
  log('[docExposure] Starting Document & CRM exposure scan for', companyName);

  if (!process.env.SERPER_KEY) {
    log('[docExposure] SERPER_KEY not found, skipping.');
    return 0;
  }

  const headers = { 'X-API-KEY': process.env.SERPER_KEY };
  const seen = new Set<string>();
  const dorksByCat = await getDorks(companyName, domain);
  let findingsCount = 0;
  let queriesExecuted = 0;

  for (const [category, dorks] of dorksByCat.entries()) {
    log(`[docExposure] Searching category: ${category}`);
    for (const query of dorks) {
      queriesExecuted++;
      try {
        log('[docExposure] Searching:', query);
        const { data } = await axios.post(SERPER_URL, { q: query, num: 20 }, { headers });

        for (const hit of data.organic ?? []) {
          const url: string = hit.link;
          if (seen.has(url)) continue;
          seen.add(url);

          const lowerTitle = (hit.title || '').toLowerCase();
          const lowerSnippet = (hit.snippet || '').toLowerCase();
          if (!lowerTitle.includes(companyName.toLowerCase()) && !lowerSnippet.includes(companyName.toLowerCase()) && !url.includes(domain)) {
              log(`[docExposure] Skipping irrelevant result: ${url}`);
              continue;
          }

          const platform = getPlatform(url);
          const analysisResult = await downloadAndAnalyze(url, companyName, scanId);

          if (analysisResult) {
            const { sha256, mimeInfo, localPath, sensitivity, findings, fileMetadata, language } = analysisResult;
            const severity = getSeverity(sensitivity);
            const key = `exposed_docs/${platform.toLowerCase()}/${sha256}${path.extname(url).split('?')[0]}`;
            const storageUrl = await uploadFile(localPath, key, mimeInfo.verified);

            const artifactId = await insertArtifact({
              type: 'exposed_document',
              val_text: `${platform} exposed file: ${path.basename(url)}`,
              severity,
              src_url: url,
              sha256,
              mime: mimeInfo.verified,
              meta: {
                scan_id: scanId,
                scan_module: 'documentExposure',
                platform,
                sensitivity_score: sensitivity,
                storage_url: storageUrl,
                file_size: (await fs.stat(localPath).catch(() => ({size:0}))).size,
                language,
                dork_category: category,
                content_analysis_summary: findings.slice(0, 5)
              }
            });

            if (sensitivity >= 15) {
                await insertFinding(artifactId, 'DATA_EXPOSURE', `Secure the ${platform} service by reviewing file permissions.`, `Sensitive document found on ${platform}. Score: ${sensitivity}.`);
            }
            findingsCount++;
          }
        }
        await new Promise(resolve => setTimeout(resolve, 1500));
      } catch (err) {
        log('[docExposure] Search loop error:', (err as Error).message);
      }
    }
  }

  log(`[docExposure] Scan complete. Found ${findingsCount} files.`);
  await insertArtifact({
    type: 'scan_summary',
    val_text: `Document exposure scan completed: ${findingsCount} exposed files found`,
    severity: 'INFO',
    meta: { scan_id: scanId, scan_module: 'documentExposure', total_findings: findingsCount, queries_executed: queriesExecuted, timestamp: new Date().toISOString() }
  });
  return findingsCount;
}
</file>

<file path="modules/endpointDiscovery.ts">
/*
 * =============================================================================
 * MODULE: endpointDiscovery.ts (Refactored v3)
 * =============================================================================
 * This module discovers web endpoints through passive, active, and
 * brute-force methods.
 *
 * Key Improvements from previous version:
 * 1.  **Authentication Probing:** Now checks for protected endpoints by sending
 * requests with an expanded list of common authentication headers.
 * 2.  **User-Agent Rotation:** Rotates through different User-Agent strings to
 * increase stealth and avoid simple blocking.
 * 3.  **Request Spacing:** Adds configurable delays between batches of
 * concurrent requests to reduce server load and detection risk.
 * 4.  **Resource Limits:** Imposes a max file size limit on downloaded JS files
 * to prevent resource exhaustion.
 * 5.  **Comprehensive Discovery:** Fully implements passive (robots.txt,
 * sitemap.xml), active (crawling, JS analysis), and brute-force discovery.
 * =============================================================================
 */

import axios from 'axios';
import { parse } from 'node-html-parser';
import { insertArtifact } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

// --- Configuration ---
const MAX_CRAWL_DEPTH = 2;
const MAX_CONCURRENT_REQUESTS = 5;
const REQUEST_TIMEOUT = 8000;
const DELAY_BETWEEN_CHUNKS_MS = 500;
const MAX_JS_FILE_SIZE_BYTES = 1024 * 1024; // 1MB limit.

const ENDPOINT_WORDLIST = ["api","admin","app","assets","auth","blog","board","cgi-bin","data","dev","docs","files","forum","img","include","js","lib","login","media","modules","news","pages","scripts","server","src","static","uploads","user","v1","v2","v3","web","wp-admin","wp-content","wp-includes","about","account","api-docs","backup","bin","config","contact","css","dashboard","db","dist","download","en","graphql","home","images","includes","index.php","json","jsp","lib","local","logs","main","phpmyadmin","private","public","register","search","services","setup","sitemap","sitemap.xml","sql","support","swagger","swagger-ui.html","system","test","upload","vendor","videos","web.config","wordpress"];

// REFACTOR: Expanded auth header coverage.
const AUTH_PROBE_HEADERS = [
    { 'Authorization': 'Bearer test' },
    { 'X-API-Key': 'test' },
    { 'x-access-token': 'test' },
    { 'X-Auth-Token': 'test' },
    { 'Cookie': 'session=test' },
    { 'X-Forwarded-User': 'test' }
];

// REFACTOR: Added User-Agent rotation for stealth.
const USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
    'curl/7.81.0',
    'python-requests/2.27.1',
    'Go-http-client/1.1'
];

interface DiscoveredEndpoint {
  url: string;
  path: string;
  confidence: 'high' | 'medium' | 'low';
  source: 'robots.txt' | 'sitemap.xml' | 'crawl_link' | 'js_analysis' | 'wordlist_enum' | 'auth_probe';
  statusCode?: number;
}

const discoveredEndpoints = new Map<string, DiscoveredEndpoint>();

// --- Helper Functions ---
function getRandomUserAgent(): string {
    return USER_AGENTS[Math.floor(Math.random() * USER_AGENTS.length)];
}

function addEndpoint(endpoint: Omit<DiscoveredEndpoint, 'url'>, baseUrl: string) {
    const fullUrl = `${baseUrl}${endpoint.path}`;
    if (!discoveredEndpoints.has(endpoint.path)) {
        log(`[endpointDiscovery] Discovered [${endpoint.source}]: ${endpoint.path} (Status: ${endpoint.statusCode || 'N/A'})`);
        discoveredEndpoints.set(endpoint.path, { ...endpoint, url: fullUrl });
    }
}

// --- Passive Discovery ---

async function parseRobotsTxt(baseUrl: string) {
    try {
        const { data } = await axios.get(`${baseUrl}/robots.txt`, { timeout: REQUEST_TIMEOUT, headers: { 'User-Agent': getRandomUserAgent() } });
        const lines = data.split('\n');
        for (const line of lines) {
            const parts = line.split(':').map((p: string) => p.trim());
            if (parts.length < 2) continue;
            const directive = parts[0].toLowerCase();
            const path = parts[1].split(' ')[0];
            if ((directive === 'disallow' || directive === 'allow') && path.startsWith('/')) {
                addEndpoint({ path, confidence: 'medium', source: 'robots.txt' }, baseUrl);
            } else if (directive === 'sitemap') {
                const sitemapUrl = new URL(parts[1], baseUrl).toString();
                await parseSitemap(sitemapUrl, baseUrl);
            }
        }
    } catch (error) {
        log(`[endpointDiscovery] Could not fetch or parse robots.txt for ${baseUrl}`);
    }
}

async function parseSitemap(sitemapUrl: string, baseUrl: string) {
    try {
        const { data } = await axios.get(sitemapUrl, { timeout: REQUEST_TIMEOUT, headers: { 'User-Agent': getRandomUserAgent() } });
        const root = parse(data);
        const locs = root.querySelectorAll('loc').map(el => el.text);
        for (const loc of locs) {
            try {
                const url = new URL(loc);
                addEndpoint({ path: url.pathname, confidence: 'high', source: 'sitemap.xml' }, baseUrl);
            } catch { /* Ignore invalid URLs */ }
        }
        const nestedSitemaps = root.querySelectorAll('sitemap > loc').map(el => el.text);
        for (const nested of nestedSitemaps) {
            await parseSitemap(nested, baseUrl);
        }
    } catch (error) {
        log(`[endpointDiscovery] Could not fetch or parse sitemap: ${sitemapUrl}`);
    }
}

// --- Active Discovery & Analysis ---

async function analyzeJsFile(jsUrl: string, baseUrl: string) {
    try {
        const { data: jsContent } = await axios.get(jsUrl, { 
            timeout: REQUEST_TIMEOUT,
            maxContentLength: MAX_JS_FILE_SIZE_BYTES,
            headers: { 'User-Agent': getRandomUserAgent() }
        });
        const pathRegex = /['"](\/[a-zA-Z0-9\/\-_.]*(api|user|auth|v1|v2|v3|graphql|jwt|token)[a-zA-Z0-9\/\-_.]*)['"]/g;
        let match;
        while ((match = pathRegex.exec(jsContent)) !== null) {
            addEndpoint({ path: match[1], confidence: 'medium', source: 'js_analysis' }, baseUrl);
        }
    } catch (error) {
        log(`[endpointDiscovery] Failed to analyze JS file: ${jsUrl}`);
    }
}

async function crawlPage(url: string, depth: number, baseUrl: string, visited: Set<string>) {
    if (depth > MAX_CRAWL_DEPTH || visited.has(url)) return;
    visited.add(url);
    log(`[endpointDiscovery] Crawling [Depth ${depth}]: ${url}`);
    try {
        const { data: html } = await axios.get(url, { timeout: REQUEST_TIMEOUT, headers: { 'User-Agent': getRandomUserAgent() } });
        const root = parse(html);
        const pageLinks = new Set<string>();
        root.querySelectorAll('a[href]').forEach(link => {
            try {
                const absoluteUrl = new URL(link.getAttribute('href')!, baseUrl).toString();
                if (absoluteUrl.startsWith(baseUrl)) {
                    addEndpoint({ path: new URL(absoluteUrl).pathname, confidence: 'low', source: 'crawl_link' }, baseUrl);
                    pageLinks.add(absoluteUrl);
                }
            } catch { /* Ignore malformed hrefs */ }
        });
        root.querySelectorAll('script[src]').forEach(script => {
            try {
                const absoluteUrl = new URL(script.getAttribute('src')!, baseUrl).toString();
                if (absoluteUrl.startsWith(baseUrl)) {
                     analyzeJsFile(absoluteUrl, baseUrl);
                }
            } catch { /* Ignore malformed srcs */ }
        });
        for (const pageLink of pageLinks) {
            await crawlPage(pageLink, depth + 1, baseUrl, visited);
        }
    } catch (error) {
        log(`[endpointDiscovery] Failed to crawl page: ${url}`);
    }
}

async function bruteForceEndpoints(baseUrl: string) {
    const promises = ENDPOINT_WORDLIST.flatMap(word => {
        const path = `/${word}`;
        const userAgentHeader = { 'User-Agent': getRandomUserAgent() };
        
        // Basic probe without authentication
        const basicProbe = {
            promise: axios.head(`${baseUrl}${path}`, { timeout: REQUEST_TIMEOUT, validateStatus: () => true, headers: userAgentHeader }),
            path,
            source: 'wordlist_enum' as const
        };
        
        // Additional probes with authentication headers
        const authProbes = AUTH_PROBE_HEADERS.map(header => ({
            promise: axios.get(`${baseUrl}${path}`, { timeout: REQUEST_TIMEOUT, headers: { ...header, ...userAgentHeader }, validateStatus: () => true }),
            path,
            source: 'auth_probe' as const
        }));
        
        return [basicProbe, ...authProbes];
    });

    for (let i = 0; i < promises.length; i += MAX_CONCURRENT_REQUESTS) {
        const chunk = promises.slice(i, i + MAX_CONCURRENT_REQUESTS);
        const results = await Promise.allSettled(chunk.map(p => p.promise));

        results.forEach((result, index) => {
            if (result.status === 'fulfilled') {
                const response = result.value;
                const { path, source } = chunk[index];
                if (response.status < 400 || response.status === 401 || response.status === 403) {
                    addEndpoint({ path, confidence: 'low', source, statusCode: response.status }, baseUrl);
                }
            }
        });
        
        await new Promise(resolve => setTimeout(resolve, DELAY_BETWEEN_CHUNKS_MS));
    }
}


// --- Main Execution Logic ---

export async function runEndpointDiscovery(job: { domain: string; scanId?: string }): Promise<number> {
    log(`[endpointDiscovery] Starting enhanced endpoint discovery for ${job.domain}`);
    const baseUrl = `https://${job.domain}`;
    discoveredEndpoints.clear();

    log('[endpointDiscovery] --- Starting Passive Discovery Phase ---');
    await parseRobotsTxt(baseUrl);
    await parseSitemap(`${baseUrl}/sitemap.xml`, baseUrl);
    
    log('[endpointDiscovery] --- Starting Active Crawling Phase ---');
    await crawlPage(baseUrl, 1, baseUrl, new Set<string>());

    log('[endpointDiscovery] --- Starting Brute-Force & Auth Probe Phase ---');
    await bruteForceEndpoints(baseUrl);

    const endpointsArray = Array.from(discoveredEndpoints.values());

    if (endpointsArray.length > 0) {
        await insertArtifact({
            type: 'discovered_endpoints',
            val_text: `Discovered ${endpointsArray.length} unique endpoints for ${job.domain}`,
            severity: 'INFO',
            meta: {
                scan_id: job.scanId,
                scan_module: 'endpointDiscovery',
                endpoints: endpointsArray
            }
        });
    }

    log(`[endpointDiscovery] Completed, found ${endpointsArray.length} unique endpoints.`);
    return endpointsArray.length;
}
</file>

<file path="modules/nuclei.ts">
/*
 * =============================================================================
 * MODULE: nuclei.ts (Refactored v2)
 * =============================================================================
 * This module runs the Nuclei vulnerability scanner against a set of targets.
 *
 * Key Improvements from previous version:
 * 1.  **Optimized Template Updates:** Checks the last update time and only updates
 * templates if they are older than 24 hours, improving efficiency.
 * 2.  **Configurable Workflow Paths:** The base path for Nuclei workflows is now
 * configurable via an environment variable for deployment flexibility.
 * 3.  **Dependency & Template Management:** Validates that the 'nuclei' binary
 * is installed and ensures templates are kept up-to-date.
 * 4.  **Workflow Execution:** Implements the ability to run advanced, multi-step
 * Nuclei workflows for specific technologies.
 * 5.  **Concurrency & Better Structure:** Scans are run in parallel and logically
 * separated into broad and deep-dive scan phases.
 * =============================================================================
 */

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import fs from 'node:fs/promises';
import path from 'node:path'; // REFACTOR: Added for path joining.
import { insertArtifact, insertFinding } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const exec = promisify(execFile);
const MAX_CONCURRENT_SCANS = 4;

const TECH_TO_NUCLEI_TAG_MAP: Record<string, string[]> = {
  "wordpress": ["wordpress", "wp-plugin", "wp-theme"],
  "joomla": ["joomla"],
  "drupal": ["drupal"],
  "nginx": ["nginx"],
  "apache": ["apache", "httpd"],
  "iis": ["iis"],
  "php": ["php"],
  "java": ["java", "tomcat", "spring", "log4j"],
  "python": ["python", "django", "flask"],
  "nodejs": ["nodejs", "express"],
  "graphql": ["graphql"],
  "elasticsearch": ["elasticsearch"],
};

// REFACTOR: Workflow base path is now configurable.
const WORKFLOW_BASE_PATH = process.env.NUCLEI_WORKFLOWS_PATH || './workflows';
const TECH_TO_WORKFLOW_MAP: Record<string, string> = {
    'wordpress': 'wordpress-workflow.yaml', // Store only the filename
    'jira': 'jira-workflow.yaml'
};

// REFACTOR: Location for tracking the last template update time.
const LAST_UPDATE_TIMESTAMP_PATH = '/tmp/nuclei_last_update.txt';

async function validateDependencies(): Promise<boolean> {
    try {
        await exec('nuclei', ['-version']);
        log('[nuclei] Nuclei binary found.');
        return true;
    } catch (error) {
        log('[nuclei] [CRITICAL] Nuclei binary not found. Scans will be skipped.');
        return false;
    }
}

/**
 * REFACTOR: Template update is now optimized. It only runs if the last update
 * was more than 24 hours ago.
 */
async function updateTemplatesIfNeeded(): Promise<void> {
    try {
        let lastUpdateTime = 0;
        try {
            const content = await fs.readFile(LAST_UPDATE_TIMESTAMP_PATH, 'utf8');
            lastUpdateTime = parseInt(content.trim()) || 0;
        } catch {
            // File doesn't exist or can't be read, treat as never updated
            lastUpdateTime = 0;
        }
        
        const oneDay = 24 * 60 * 60 * 1000;

        if (Date.now() - lastUpdateTime > oneDay) {
            log('[nuclei] Templates are outdated (> 24 hours). Updating...');
            await exec('nuclei', ['-update-templates'], { timeout: 300000 }); // 5 min timeout
            await fs.writeFile(LAST_UPDATE_TIMESTAMP_PATH, Date.now().toString());
            log('[nuclei] Template update complete.');
        } else {
            log('[nuclei] Templates are up-to-date. Skipping update.');
        }
    } catch (error) {
        log('[nuclei] [WARNING] Failed to update nuclei templates. Scans will proceed with local version.', (error as Error).message);
    }
}


async function processNucleiOutput(stdout: string, scanId: string, scanType: 'tags' | 'workflow', workflowFile?: string) {
    const findings = stdout.trim().split('\n').filter(Boolean);
    for (const line of findings) {
        try {
            const vuln = JSON.parse(line);
            const severity = (vuln.info.severity.toUpperCase() as any) || 'INFO';

            const artifactId = await insertArtifact({
                type: 'vuln',
                val_text: `${vuln.info.name} on ${vuln.host}`,
                severity,
                src_url: vuln.host,
                meta: {
                    scan_id: scanId,
                    scan_module: 'nuclei',
                    scan_type: scanType,
                    template_id: vuln['template-id'],
                    workflow_file: workflowFile,
                    vulnerability: vuln.info,
                    'curl-command': vuln['curl-command'],
                    'matcher-status': vuln['matcher-status'],
                    'extracted-results': vuln['extracted-results'],
                }
            });
            await insertFinding(artifactId, 'VULNERABILITY', 'See artifact details and Nuclei template for remediation guidance.', vuln.info.description);
        } catch (e) {
            log(`[nuclei] Failed to parse result line:`, line);
        }
    }
    return findings.length;
}


async function runNucleiTagScan(target: { url: string; tech?: string[] }, scanId?: string): Promise<number> {
    const baseTags = new Set(['cve', 'misconfiguration', 'default-logins', 'exposed-panels', 'exposure', 'tech']);
    if (target.tech) {
        for (const tech of target.tech) {
            const tags = TECH_TO_NUCLEI_TAG_MAP[tech.toLowerCase()];
            if (tags) tags.forEach(tag => baseTags.add(tag));
        }
    }
    const tags = Array.from(baseTags).join(',');

    log(`[nuclei] [Tag Scan] Running on ${target.url} with tags: ${tags}`);
    try {
        const { stdout } = await exec('nuclei', [
            '-u', target.url,
            '-tags', tags,
            '-json',
            '-silent',
            '-timeout', '10',
            '-retries', '2',
            '-headless'
        ], { timeout: 600000 });

        return await processNucleiOutput(stdout, scanId!, 'tags');
    } catch (error) {
        log(`[nuclei] [Tag Scan] Failed for ${target.url}:`, (error as Error).message);
        return 0;
    }
}


async function runNucleiWorkflow(target: { url: string }, workflowFileName: string, scanId?: string): Promise<number> {
    // REFACTOR: Construct full path from base path and filename.
    const workflowPath = path.join(WORKFLOW_BASE_PATH, workflowFileName);
    
    log(`[nuclei] [Workflow Scan] Running workflow '${workflowPath}' on ${target.url}`);
    
    try {
        await fs.access(workflowPath);
    } catch {
        log(`[nuclei] [Workflow Scan] SKIPPING: Workflow file not found at ${workflowPath}`);
        return 0;
    }

    try {
        const { stdout } = await exec('nuclei', [
            '-u', target.url,
            '-w', workflowPath,
            '-json',
            '-silent',
            '-timeout', '15'
        ], { timeout: 900000 });

        return await processNucleiOutput(stdout, scanId!, 'workflow', workflowPath);
    } catch (error) {
        log(`[nuclei] [Workflow Scan] Failed for ${target.url} with workflow ${workflowPath}:`, (error as Error).message);
        return 0;
    }
}

export async function runNuclei(job: { domain: string; scanId?: string; targets?: { url: string; tech?: string[] }[] }): Promise<number> {
    log('[nuclei] Starting enhanced vulnerability scan for', job.domain);
    
    if (!(await validateDependencies())) {
        await insertArtifact({type: 'scan_error', val_text: 'Nuclei binary not found, scan aborted.', severity: 'HIGH', meta: { scan_id: job.scanId, scan_module: 'nuclei' }});
        return 0;
    }
    // REFACTOR: Call the optimized update function.
    await updateTemplatesIfNeeded();

    let totalFindings = 0;
    const targets = job.targets?.length ? job.targets : [{ url: `https://${job.domain}` }];
    
    log(`[nuclei] --- Starting Phase 1: Tag-based scans on ${targets.length} targets ---`);
    for (let i = 0; i < targets.length; i += MAX_CONCURRENT_SCANS) {
        const chunk = targets.slice(i, i + MAX_CONCURRENT_SCANS);
        const results = await Promise.all(chunk.map(target => runNucleiTagScan(target, job.scanId)));
        totalFindings += results.reduce((a, b) => a + b, 0);
    }

    log(`[nuclei] --- Starting Phase 2: Deep-Dive Workflow Scans ---`);
    for (const target of targets) {
        const detectedTech = new Set(target.tech?.map(t => t.toLowerCase()) || []);
        for (const tech in TECH_TO_WORKFLOW_MAP) {
            if (detectedTech.has(tech)) {
                // REFACTOR: Pass the workflow filename, not the full path.
                totalFindings += await runNucleiWorkflow(target, TECH_TO_WORKFLOW_MAP[tech], job.scanId);
            }
        }
    }

    log(`[nuclei] Completed vulnerability scan. Total findings: ${totalFindings}`);
    await insertArtifact({
        type: 'scan_summary',
        val_text: `Nuclei scan completed: ${totalFindings} vulnerabilities found`,
        severity: 'INFO',
        meta: {
            scan_id: job.scanId,
            scan_module: 'nuclei',
            total_findings: totalFindings,
            targets_scanned: targets.length,
            timestamp: new Date().toISOString()
        }
    });
    
    return totalFindings;
}
</file>

<file path="modules/rateLimitScan.ts">
/*
 * =============================================================================
 * MODULE: rateLimitScan.ts (Consolidated & Refactored)
 * =============================================================================
 * This module replaces zapRateIp.ts, zapRateTest.ts, and zapRateToken.ts
 * with a single, comprehensive rate limit testing engine.
 *
 * Key Improvements:
 * 1.  **Integrated Endpoint Discovery:** Uses the output from the endpointDiscovery
 * module to find the best targets (login, API, auth endpoints) for testing.
 * 2.  **Structured Testing:** Establishes a baseline to confirm a rate limit
 * exists before attempting a wide range of bypass techniques.
 * 3.  **Expanded Bypass Techniques:** Tests for bypasses via IP spoofing headers,
 * HTTP method switching, path variations, and parameter pollution.
 * 4.  **Consolidated Findings:** Groups all successful bypass methods for a
 * single endpoint into one actionable artifact.
 * =============================================================================
 */

import axios, { Method } from 'axios';
import { insertArtifact, insertFinding, pool } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const REQUEST_BURST_COUNT = 25; // Number of requests to send to trigger a baseline limit.
const REQUEST_TIMEOUT = 5000;

interface DiscoveredEndpoint {
  url: string;
  path: string;
  method?: string; // Original method, may not be present
}

interface RateLimitTestResult {
    bypassed: boolean;
    technique: string;
    details: string;
    statusCode?: number;
}

const IP_SPOOFING_HEADERS = [
    { 'X-Forwarded-For': '127.0.0.1' }, { 'X-Real-IP': '127.0.0.1' },
    { 'X-Client-IP': '127.0.0.1' }, { 'X-Originating-IP': '127.0.0.1' },
    { 'X-Remote-IP': '127.0.0.1' }, { 'Forwarded': 'for=127.0.0.1' },
    { 'X-Forwarded': '127.0.0.1' }, { 'Forwarded-For': '127.0.0.1' },
];

/**
 * Fetches interesting endpoints discovered by other modules.
 */
async function getTestableEndpoints(scanId: string, domain: string): Promise<DiscoveredEndpoint[]> {
    try {
        const result = await pool.query(
            `SELECT meta FROM artifacts WHERE type = 'discovered_endpoints' AND meta->>'scan_id' = $1 LIMIT 1`,
            [scanId]
        );
        if (result.rows.length > 0 && result.rows[0].meta.endpoints) {
            const endpoints = result.rows[0].meta.endpoints as DiscoveredEndpoint[];
            // Filter for endpoints most likely to have rate limits
            return endpoints.filter(e => 
                e.path.includes('login') || e.path.includes('register') || 
                e.path.includes('auth') || e.path.includes('api') || e.path.includes('password')
            );
        }
    } catch (e) {
        log('[rateLimitScan] [ERROR] Could not fetch endpoints from database:', (e as Error).message);
    }
    // Fallback if no discovered endpoints are found
    log('[rateLimitScan] No discovered endpoints found, using fallback list.');
    return [
        { url: `https://${domain}/login`, path: '/login' },
        { url: `https://${domain}/api/login`, path: '/api/login' },
        { url: `https://${domain}/auth/login`, path: '/auth/login' },
        { url: `https://${domain}/password/reset`, path: '/password/reset' },
    ];
}

/**
 * Sends a burst of requests to establish a baseline and see if a rate limit is triggered.
 * Now includes inter-burst delays and full response distribution analysis.
 */
async function establishBaseline(endpoint: DiscoveredEndpoint): Promise<{ hasRateLimit: boolean; responseDistribution: Record<number, number> }> {
    log(`[rateLimitScan] Establishing baseline for ${endpoint.url}...`);
    
    const responseDistribution: Record<number, number> = {};
    const chunkSize = 5; // Send requests in smaller chunks
    const interBurstDelay = 100; // 100ms delay between chunks
    
    for (let chunk = 0; chunk < REQUEST_BURST_COUNT / chunkSize; chunk++) {
        const promises = [];
        
        // Send chunk of requests
        for (let i = 0; i < chunkSize; i++) {
            promises.push(
                axios.post(endpoint.url, {u:'test',p:'test'}, { 
                    timeout: REQUEST_TIMEOUT, 
                    validateStatus: () => true 
                }).catch(error => ({ 
                    status: error.response?.status || 0 
                }))
            );
        }
        
        const responses = await Promise.allSettled(promises);
        
        // Collect response status codes
        for (const response of responses) {
            if (response.status === 'fulfilled') {
                const statusCode = response.value.status;
                responseDistribution[statusCode] = (responseDistribution[statusCode] || 0) + 1;
            }
        }
        
        // Add delay between chunks (except for the last chunk)
        if (chunk < (REQUEST_BURST_COUNT / chunkSize) - 1) {
            await new Promise(resolve => setTimeout(resolve, interBurstDelay));
        }
    }
    
    log(`[rateLimitScan] Response distribution for ${endpoint.url}:`, responseDistribution);
    
    // Analyze the response distribution to determine if rate limiting is present
    const has429 = responseDistribution[429] > 0;
    const hasProgressiveFailure = Object.keys(responseDistribution).length > 2; // Multiple status codes suggest rate limiting
    const successRate = (responseDistribution[200] || 0) / REQUEST_BURST_COUNT;
    
    // Rate limiting is likely present if:
    // 1. We got 429 responses, OR
    // 2. We have progressive failure patterns (multiple status codes), OR  
    // 3. Success rate drops significantly (< 80%)
    const hasRateLimit = has429 || hasProgressiveFailure || successRate < 0.8;
    
    return { hasRateLimit, responseDistribution };
}

/**
 * Attempts to bypass a rate limit using various techniques.
 * Now includes delays between bypass attempts to avoid interference.
 */
async function testBypassTechniques(endpoint: DiscoveredEndpoint): Promise<RateLimitTestResult[]> {
    const results: RateLimitTestResult[] = [];
    const testPayload = { user: 'testuser', pass: 'testpass' };
    const bypassDelay = 200; // 200ms delay between bypass attempts

    // 1. IP Spoofing Headers
    for (const header of IP_SPOOFING_HEADERS) {
        try {
            const response = await axios.post(endpoint.url, testPayload, { 
                headers: header, 
                timeout: REQUEST_TIMEOUT, 
                validateStatus: () => true 
            });
            if (response.status !== 429) {
                results.push({ 
                    bypassed: true, 
                    technique: 'IP_SPOOFING_HEADER', 
                    details: `Header: ${Object.keys(header)[0]}`, 
                    statusCode: response.status 
                });
            }
            await new Promise(resolve => setTimeout(resolve, bypassDelay));
        } catch { /* ignore */ }
    }

    // 2. HTTP Method Switching
    try {
        const response = await axios.get(endpoint.url, { 
            params: testPayload, 
            timeout: REQUEST_TIMEOUT, 
            validateStatus: () => true 
        });
        if (response.status !== 429) {
            results.push({ 
                bypassed: true, 
                technique: 'HTTP_METHOD_SWITCH', 
                details: 'Used GET instead of POST', 
                statusCode: response.status 
            });
        }
        await new Promise(resolve => setTimeout(resolve, bypassDelay));
    } catch { /* ignore */ }
    
    // 3. Path Variation
    for (const path of [`${endpoint.path}/`, `${endpoint.path}.json`, endpoint.path.toUpperCase()]) {
        try {
            const url = new URL(endpoint.url);
            url.pathname = path;
            const response = await axios.post(url.toString(), testPayload, { 
                timeout: REQUEST_TIMEOUT, 
                validateStatus: () => true 
            });
            if (response.status !== 429) {
                results.push({ 
                    bypassed: true, 
                    technique: 'PATH_VARIATION', 
                    details: `Path used: ${path}`, 
                    statusCode: response.status 
                });
            }
            await new Promise(resolve => setTimeout(resolve, bypassDelay));
        } catch { /* ignore */ }
    }

    return results;
}

export async function runRateLimitScan(job: { domain: string, scanId: string }): Promise<number> {
    log('[rateLimitScan] Starting comprehensive rate limit scan for', job.domain);
    let findingsCount = 0;

    const endpoints = await getTestableEndpoints(job.scanId, job.domain);
    if (endpoints.length === 0) {
        log('[rateLimitScan] No testable endpoints found. Skipping.');
        return 0;
    }

    log(`[rateLimitScan] Found ${endpoints.length} endpoints to test.`);

    for (const endpoint of endpoints) {
        const { hasRateLimit, responseDistribution } = await establishBaseline(endpoint);

        if (!hasRateLimit) {
            log(`[rateLimitScan] No baseline rate limit detected on ${endpoint.url}.`);
            const artifactId = await insertArtifact({
                type: 'rate_limit_missing',
                val_text: `No rate limiting detected on endpoint: ${endpoint.path}`,
                severity: 'MEDIUM',
                src_url: endpoint.url,
                meta: { 
                    scan_id: job.scanId, 
                    scan_module: 'rateLimitScan', 
                    endpoint: endpoint.path,
                    response_distribution: responseDistribution
                }
            });
            await insertFinding(artifactId, 'MISSING_RATE_LIMITING', `Implement strict rate limiting on this endpoint (${endpoint.path}) to prevent brute-force attacks.`, `The endpoint did not show rate limiting behavior after ${REQUEST_BURST_COUNT} rapid requests. Response distribution: ${JSON.stringify(responseDistribution)}`);
            findingsCount++;
            continue;
        }

        log(`[rateLimitScan] Baseline rate limit detected on ${endpoint.url}. Testing for bypasses...`);
        
        // Wait a bit before testing bypasses to let any rate limits reset
        await new Promise(resolve => setTimeout(resolve, 2000));
        
        const bypassResults = await testBypassTechniques(endpoint);
        const successfulBypasses = bypassResults.filter(r => r.bypassed);

        if (successfulBypasses.length > 0) {
            log(`[rateLimitScan] [VULNERABLE] Found ${successfulBypasses.length} bypass techniques for ${endpoint.url}`);
            const artifactId = await insertArtifact({
                type: 'rate_limit_bypass',
                val_text: `Rate limit bypass possible on endpoint: ${endpoint.path}`,
                severity: 'HIGH',
                src_url: endpoint.url,
                meta: {
                    scan_id: job.scanId,
                    scan_module: 'rateLimitScan',
                    endpoint: endpoint.path,
                    bypasses: successfulBypasses,
                    baseline_distribution: responseDistribution
                }
            });
            await insertFinding(artifactId, 'RATE_LIMIT_BYPASS', `The rate limiting implementation on ${endpoint.path} can be bypassed. Ensure that the real client IP is correctly identified and that logic is not easily evaded by simple transformations.`, `Successful bypass techniques: ${successfulBypasses.map(b => b.technique).join(', ')}.`);
            findingsCount++;
        } else {
            log(`[rateLimitScan] Rate limiting on ${endpoint.url} appears to be robust.`);
        }
    }

    await insertArtifact({
        type: 'scan_summary',
        val_text: `Rate limit scan completed: ${findingsCount} issues found`,
        severity: 'INFO',
        meta: {
            scan_id: job.scanId,
            scan_module: 'rateLimitScan',
            total_findings: findingsCount,
            endpoints_tested: endpoints.length,
            timestamp: new Date().toISOString()
        }
    });

    return findingsCount;
}
</file>

<file path="modules/shodan.ts">
/*
 * =============================================================================
 * MODULE: shodan.ts (Refactored)
 * =============================================================================
 * This module uses the Shodan API to find exposed services and vulnerabilities
 * associated with a target domain and organization.
 *
 * Key Improvements from previous version:
 * 1.  **More Comprehensive Target List:** The number of subdomains and IPs pulled
 * from previous scan phases (e.g., spiderFoot) has been increased from 20
 * to 100 for more thorough scanning of larger organizations.
 * 2.  **Context-Aware Recommendations:** The recommendation engine is no longer
 * generic. It now provides specific, actionable advice based on the
 * discovered service, port, and version, including tailored hardening guides
 * and patch notifications for known CVEs.
 * =============================================================================
 */

import axios from 'axios';
import { insertArtifact, insertFinding, pool } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

interface ShodanResult {
  ip_str: string;
  port: number;
  location: {
    country_name?: string;
    city?: string;
  };
  org?: string;
  isp?: string;
  product?: string;
  version?: string;
  // REFACTOR: vulns is an object with CVEs as keys for better structure.
  vulns?: Record<string, any>;
  ssl?: {
    cert?: {
      subject?: {
        CN?: string;
      };
      issuer?: {
        CN?: string;
      };
      expired?: boolean;
    };
  };
  http?: {
    title?: string;
    server?: string;
  };
  banner?: string;
  hostnames?: string[];
}

interface ShodanResponse {
  matches: ShodanResult[];
  total: number;
}

/**
 * REFACTOR: The recommendation function is now much more dynamic and context-aware.
 * It generates specific advice based on the service, version, and finding.
 */
function getShodanRecommendation(port: number, serviceInfo: { product: string; version: string; }, finding: string): string {
  const recommendations: Record<number, string> = {
    21: 'Disable FTP or enforce FTPS with strong authentication.',
    22: 'Secure SSH with key-based auth, disable password auth, use fail2ban, and consider changing the default port.',
    23: 'CRITICAL: Disable Telnet immediately. It is an insecure plaintext protocol. Use SSH instead.',
    25: 'Secure SMTP with modern authentication (SPF, DKIM, DMARC) and enforce STARTTLS.',
    53: 'Secure DNS server against cache poisoning and amplification attacks. Use DNSSEC if possible.',
    80: 'Migrate to HTTPS (port 443) and redirect all HTTP traffic. Implement HSTS.',
    110: 'Use POP3S (port 995) instead of plaintext POP3.',
    135: 'Block RPC ports from internet access. This is a common vector for worms.',
    139: 'Block NetBIOS/SMB ports from internet access. This is a critical security risk.',
    445: 'Block SMB from internet access. This is a critical security risk and a common ransomware vector.',
    143: 'Use IMAPS (port 993) instead of plaintext IMAP.',
    1433: 'Block SQL Server access from the internet. Access should be via a VPN or bastion host.',
    1521: 'Block Oracle DB access from the internet. Access should be via a VPN or bastion host.',
    3306: 'Block MySQL/MariaDB access from the internet. Access should be via a VPN or bastion host.',
    3389: 'Block RDP from the internet or protect it with a Gateway and Multi-Factor Authentication.',
    5432: 'Block PostgreSQL access from the internet. Access should be via a VPN or bastion host.',
    5900: 'Block VNC from the internet. It is often unencrypted. Use a secure remote access solution like SSH tunneling or a VPN.',
    6379: 'Block Redis from the internet. Enable password authentication and run in protected mode.',
    9200: 'Block Elasticsearch from the internet. Use authentication and role-based access control.',
  };

  if (finding.includes('vulnerability')) {
    const cve = finding.split(':')[1]?.trim();
    if (cve) {
      return `CRITICAL: Immediately patch the identified vulnerability ${cve} for ${serviceInfo.product} ${serviceInfo.version}. Review vendor advisories for mitigation steps.`;
    }
    return `CRITICAL: Immediately investigate and patch all identified vulnerabilities for ${serviceInfo.product} ${serviceInfo.version}.`;
  }
  
  if(finding.includes('Expired SSL certificate')) {
      return 'Renew the SSL/TLS certificate immediately to prevent trust errors and potential security warnings for users.';
  }

  return recommendations[port] || `Review the configuration for ${serviceInfo.product} on port ${port} and restrict internet access if it's not required. Follow security best practices for this service.`;
}


/**
 * REFACTOR: Increased the limit to 100 to get a more comprehensive list of targets.
 */
async function getDiscoveredTargets(scanId: string): Promise<string[]> {
  log('[shodan] Querying database for discovered targets...');
  try {
    const subdomainQuery = `
      SELECT DISTINCT val_text 
      FROM artifacts 
      WHERE meta->>'scan_id' = $1 
      AND type IN ('subdomain', 'ip', 'INTERNET_NAME', 'AFFILIATE_INTERNET_NAME')
      AND val_text IS NOT NULL
      LIMIT 100
    `;
    const result = await pool.query(subdomainQuery, [scanId]);
    const targets = result.rows.map(row => row.val_text.trim());
    log(`[shodan] Found ${targets.length} discovered targets from previous scans.`);
    return targets;
  } catch (error) {
    log('[shodan] [ERROR] Failed to get discovered targets from database:', (error as Error).message);
    return [];
  }
}

async function processShodanResults(matches: ShodanResult[], scanId: string, companyName: string, searchTarget: string): Promise<number> {
  let findingsCount = 0;
  for (const result of matches) {
    findingsCount++;
    const serviceInfo = {
      ip: result.ip_str,
      port: result.port,
      product: result.product || 'Unknown',
      version: result.version || 'Unknown',
      organization: result.org || 'Unknown',
      isp: result.isp || 'Unknown',
      location: result.location?.city && result.location?.country_name 
        ? `${result.location.city}, ${result.location.country_name}`
        : 'Unknown',
      banner: result.banner || '',
      hostnames: result.hostnames || [],
      search_target: searchTarget
    };

    let severity: 'INFO' | 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' = 'INFO';
    const findingsList: string[] = [];
    
    const criticalPorts = [23, 139, 445, 3389, 5900];
    const highPorts = [21, 22, 1433, 1521, 3306, 5432, 6379, 9200];
    
    if (criticalPorts.includes(result.port)) {
        severity = 'HIGH';
    } else if (highPorts.includes(result.port)) {
        severity = 'MEDIUM';
    }

    if (result.vulns && Object.keys(result.vulns).length > 0) {
      severity = 'CRITICAL';
      for (const cve of Object.keys(result.vulns)) {
        findingsList.push(`Known vulnerability: ${cve}`);
      }
    }

    if (result.ssl?.cert?.expired) {
      severity = severity === 'INFO' ? 'LOW' : severity === 'MEDIUM' ? 'MEDIUM' : 'HIGH'; // Elevate severity
      findingsList.push('Expired SSL certificate detected');
    }

    const artifactId = await insertArtifact({
      type: 'shodan_service',
      val_text: `${result.ip_str}:${result.port} - ${serviceInfo.product} ${serviceInfo.version}`,
      severity,
      src_url: `https://www.shodan.io/host/${result.ip_str}`,
      meta: {
        scan_id: scanId,
        company: companyName,
        service_info: serviceInfo,
        shodan_vulns: result.vulns,
        scan_module: 'shodan'
      }
    });

    if (findingsList.length === 0) {
        findingsList.push(`Exposed service on port ${result.port}`);
    }

    for (const finding of findingsList) {
      await insertFinding(
        artifactId,
        'EXPOSED_SERVICE',
        // REFACTOR: Pass the full serviceInfo object to the recommendation engine.
        getShodanRecommendation(result.port, serviceInfo, finding),
        finding
      );
    }
  }
  return findingsCount;
}

async function searchByOrganization(companyName: string, scanId: string, apiKey: string): Promise<number> {
  const orgSearchUrl = `https://api.shodan.io/shodan/host/search?key=${apiKey}&query=org:"${companyName}"`;
  try {
    const orgResponse = await axios.get<ShodanResponse>(orgSearchUrl, { timeout: 30000 });
    log(`[shodan] Found ${orgResponse.data.matches.length} results for organization "${companyName}"`);
    // Limit processing for org searches to avoid noise, but get a good sample.
    return await processShodanResults(orgResponse.data.matches.slice(0, 50), scanId, companyName, `org:"${companyName}"`);
  } catch (orgError) {
    log('[shodan] Organization search failed:', (orgError as Error).message);
    return 0;
  }
}

export async function runShodanScan(job: { domain: string, scanId: string, companyName: string }): Promise<number> {
    const { domain, scanId, companyName } = job;
    const apiKey = process.env.SHODAN_API_KEY;
    if (!apiKey) {
        log('[shodan] [CRITICAL] SHODAN_API_KEY not found. Scan skipped.');
        await insertArtifact({
            type: 'scan_error',
            val_text: 'Shodan scan skipped - API key not configured',
            severity: 'HIGH',
            meta: { scan_id: scanId, scan_module: 'shodan' }
        });
        return 0;
    }

    log(`[shodan] Starting comprehensive scan for domain: ${domain}, company: ${companyName}`);
    let totalFindings = 0;
    const allTargets = new Set<string>([domain]);

    try {
        const discoveredTargets = await getDiscoveredTargets(scanId);
        discoveredTargets.forEach(t => allTargets.add(t));
        
        log(`[shodan] Scanning ${allTargets.size} unique targets.`);

        for (const target of allTargets) {
            const searchUrl = `https://api.shodan.io/shodan/host/search?key=${apiKey}&query=hostname:${target}`;
            try {
                log(`[shodan] Querying for: ${target}`);
                const response = await axios.get<ShodanResponse>(searchUrl, { timeout: 30000 });
                totalFindings += await processShodanResults(response.data.matches, scanId, companyName, target);
                await new Promise(resolve => setTimeout(resolve, 1000));
            } catch (error) {
                log(`[shodan] [ERROR] Search failed for ${target}:`, (error as Error).message);
            }
        }

        if (companyName && companyName !== domain) {
            totalFindings += await searchByOrganization(companyName, scanId, apiKey);
        }

        log(`[shodan] Scan completed. Total findings: ${totalFindings}`);
        await insertArtifact({
            type: 'scan_summary',
            val_text: `Shodan scan completed: ${totalFindings} findings`,
            severity: 'INFO',
            meta: {
                scan_id: scanId,
                scan_module: 'shodan',
                total_findings: totalFindings,
                targets_scanned: allTargets.size,
                timestamp: new Date().toISOString()
            }
        });
        return totalFindings;
    } catch (error) {
        log('[shodan] [CRITICAL] Scan failed with unexpected error:', (error as Error).message);
        await insertArtifact({
            type: 'scan_error',
            val_text: `Shodan scan failed: ${(error as Error).message}`,
            severity: 'HIGH',
            meta: { scan_id: scanId, scan_module: 'shodan' }
        });
        return 0;
    }
}
</file>

<file path="modules/spfDmarc.ts">
/*
 * =============================================================================
 * MODULE: spfDmarc.ts (Refactored)
 * =============================================================================
 * This module performs deep analysis of a domain's email security posture by
 * checking DMARC, SPF, and DKIM configurations.
 *
 * Key Improvements from previous version:
 * 1.  **Recursive SPF Validation:** The SPF check now recursively resolves `include`
 * and `redirect` mechanisms to accurately count DNS lookups.
 * 2.  **Comprehensive DKIM Probing:** Probes for a much wider array of common and
 * provider-specific DKIM selectors.
 * 3.  **BIMI Record Check:** Adds validation for Brand Indicators for Message
 * Identification (BIMI) for enhanced brand trust in email clients.
 * =============================================================================
 */

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import { insertArtifact, insertFinding } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const exec = promisify(execFile);

interface SpfResult {
  record: string;
  lookups: number;
  error?: 'TOO_MANY_LOOKUPS' | 'REDIRECT_LOOP' | 'MULTIPLE_RECORDS' | 'NONE_FOUND';
  allMechanism: '~all' | '-all' | '?all' | 'none';
}

/**
 * REFACTOR: A new recursive function to fully resolve an SPF record.
 * It follows includes and redirects to accurately count DNS lookups.
 */
async function resolveSpfRecord(domain: string, lookups: number = 0, redirectChain: string[] = []): Promise<SpfResult> {
  const MAX_LOOKUPS = 10;

  if (lookups > MAX_LOOKUPS) {
    return { record: '', lookups, error: 'TOO_MANY_LOOKUPS', allMechanism: 'none' };
  }
  if (redirectChain.includes(domain)) {
    return { record: '', lookups, error: 'REDIRECT_LOOP', allMechanism: 'none' };
  }

  try {
    const { stdout } = await exec('dig', ['TXT', domain, '+short'], { timeout: 10000 });
    const records = stdout.trim().split('\n').map(s => s.replace(/"/g, '')).filter(r => r.startsWith('v=spf1'));

    if (records.length === 0) return { record: '', lookups, error: 'NONE_FOUND', allMechanism: 'none' };
    if (records.length > 1) return { record: records.join(' | '), lookups, error: 'MULTIPLE_RECORDS', allMechanism: 'none' };

    const record = records[0];
    const mechanisms = record.split(' ').slice(1);
    let currentLookups = lookups;
    let finalResult: SpfResult = { record, lookups, allMechanism: 'none' };

    for (const mech of mechanisms) {
      if (mech.startsWith('include:')) {
        currentLookups++;
        const includeDomain = mech.split(':')[1];
        const result = await resolveSpfRecord(includeDomain, currentLookups, [...redirectChain, domain]);
        currentLookups = result.lookups;
        if (result.error) return { ...finalResult, error: result.error, lookups: currentLookups };
      } else if (mech.startsWith('redirect=')) {
        currentLookups++;
        const redirectDomain = mech.split('=')[1];
        return resolveSpfRecord(redirectDomain, currentLookups, [...redirectChain, domain]);
      } else if (mech.startsWith('a') || mech.startsWith('mx') || mech.startsWith('exists:')) {
        currentLookups++;
      }
    }

    finalResult.lookups = currentLookups;
    if (record.includes('-all')) finalResult.allMechanism = '-all';
    else if (record.includes('~all')) finalResult.allMechanism = '~all';
    else if (record.includes('?all')) finalResult.allMechanism = '?all';

    if (currentLookups > MAX_LOOKUPS) {
        finalResult.error = 'TOO_MANY_LOOKUPS';
    }

    return finalResult;
  } catch (error) {
    return { record: '', lookups, error: 'NONE_FOUND', allMechanism: 'none' };
  }
}

export async function runSpfDmarc(job: { domain: string; scanId?: string }): Promise<number> {
  log('[spfDmarc] Starting email security scan for', job.domain);
  let findingsCount = 0;

  // --- 1. DMARC Check (Existing logic is good) ---
  log('[spfDmarc] Checking DMARC record...');
  try {
    const { stdout: dmarcOut } = await exec('dig', ['txt', `_dmarc.${job.domain}`, '+short']);
    if (!dmarcOut.trim()) {
        const artifactId = await insertArtifact({ type: 'dmarc_missing', val_text: `DMARC record missing`, severity: 'MEDIUM', meta: { scan_id: job.scanId, scan_module: 'spfDmarc' } });
        await insertFinding(artifactId, 'EMAIL_SECURITY_GAP', 'Implement a DMARC policy (start with p=none) to gain visibility into email channels and begin protecting against spoofing.', 'No DMARC record found.');
        findingsCount++;
    } else if (/p=none/i.test(dmarcOut)) {
        const artifactId = await insertArtifact({ type: 'dmarc_weak', val_text: `DMARC policy is not enforcing`, severity: 'LOW', meta: { record: dmarcOut.trim(), scan_id: job.scanId, scan_module: 'spfDmarc' } });
        await insertFinding(artifactId, 'EMAIL_SECURITY_WEAKNESS', 'Strengthen DMARC policy from p=none to p=quarantine or p=reject to actively prevent email spoofing.', 'DMARC policy is in monitoring mode (p=none) and provides no active protection.');
        findingsCount++;
    }
  } catch (e) {
      log('[spfDmarc] DMARC check failed or no record found.');
  }

  // --- 2. Recursive SPF Check ---
  log('[spfDmarc] Performing recursive SPF check...');
  const spfResult = await resolveSpfRecord(job.domain);
  
  if (spfResult.error === 'NONE_FOUND') {
      const artifactId = await insertArtifact({ type: 'spf_missing', val_text: `SPF record missing`, severity: 'MEDIUM', meta: { scan_id: job.scanId, scan_module: 'spfDmarc' } });
      await insertFinding(artifactId, 'EMAIL_SECURITY_GAP', 'Implement an SPF record to specify all authorized mail servers. This is a foundational step for DMARC.', 'No SPF record found.');
      findingsCount++;
  } else if (spfResult.error) {
      const artifactId = await insertArtifact({ type: 'spf_invalid', val_text: `SPF record is invalid: ${spfResult.error}`, severity: 'HIGH', meta: { record: spfResult.record, lookups: spfResult.lookups, error: spfResult.error, scan_id: job.scanId, scan_module: 'spfDmarc' } });
      await insertFinding(artifactId, 'EMAIL_SECURITY_MISCONFIGURATION', `Correct the invalid SPF record. The error '${spfResult.error}' can cause email delivery failures for legitimate mail.`, `SPF record validation failed with error: ${spfResult.error}.`);
      findingsCount++;
  } else {
    if (spfResult.allMechanism === '~all' || spfResult.allMechanism === '?all') {
        const artifactId = await insertArtifact({ type: 'spf_weak', val_text: `SPF policy is too permissive (${spfResult.allMechanism})`, severity: 'LOW', meta: { record: spfResult.record, scan_id: job.scanId, scan_module: 'spfDmarc' } });
        await insertFinding(artifactId, 'EMAIL_SECURITY_WEAKNESS', 'Strengthen SPF policy by using "-all" (hard fail) instead of "~all" (soft fail) or "?all" (neutral).', 'The SPF record does not instruct receivers to reject unauthorized mail.');
        findingsCount++;
    }
  }
  
  // --- 3. Comprehensive DKIM Check ---
  log('[spfDmarc] Probing for common DKIM selectors...');
  // REFACTOR: Expanded list of provider-specific DKIM selectors.
  const currentYear = new Date().getFullYear();
  const commonSelectors = [
      'default', 'selector1', 'selector2', 'google', 'k1', 'k2', 'mandrill', 
      'sendgrid', 'mailgun', 'zoho', 'amazonses', 'dkim', 'm1', 'pm', 'o365',
      'mailchimp', 'constantcontact', 'hubspot', 'salesforce', // Added providers
      `s${currentYear}`, `s${currentYear - 1}`
  ];
  let dkimFound = false;
  
  for (const selector of commonSelectors) {
    try {
      const { stdout: dkimOut } = await exec('dig', ['txt', `${selector}._domainkey.${job.domain}`, '+short']);
      if (dkimOut.trim().includes('k=rsa')) {
        dkimFound = true;
        log(`[spfDmarc] Found DKIM record with selector: ${selector}`);
        break;
      }
    } catch (dkimError) { /* Selector does not exist */ }
  }
  
  if (!dkimFound) {
    const artifactId = await insertArtifact({ type: 'dkim_missing', val_text: `DKIM record not detected for common selectors`, severity: 'LOW', meta: { selectors_checked: commonSelectors, scan_id: job.scanId, scan_module: 'spfDmarc' } });
    await insertFinding(artifactId, 'EMAIL_SECURITY_GAP', 'Implement DKIM signing for outbound email to cryptographically verify message integrity. This is a critical component for DMARC alignment.', 'Could not find a valid DKIM record using a wide range of common selectors.');
    findingsCount++;
  }

  // REFACTOR: --- 4. BIMI Check (Optional Enhancement) ---
  log('[spfDmarc] Checking for BIMI record...');
  try {
      const { stdout: bimiOut } = await exec('dig', ['txt', `default._bimi.${job.domain}`, '+short']);
      if (bimiOut.trim().startsWith('v=BIMI1')) {
          log(`[spfDmarc] Found BIMI record: ${bimiOut.trim()}`);
          await insertArtifact({
              type: 'bimi_found',
              val_text: 'BIMI record is properly configured',
              severity: 'INFO',
              meta: { record: bimiOut.trim(), scan_id: job.scanId, scan_module: 'spfDmarc' }
          });
      } else {
          // A missing BIMI record is not a security failure, but an opportunity.
          await insertArtifact({
              type: 'bimi_missing',
              val_text: 'BIMI record not found',
              severity: 'INFO',
              meta: { scan_id: job.scanId, scan_module: 'spfDmarc' }
          });
      }
  } catch (bimiError) {
      log('[spfDmarc] BIMI check failed or no record found.');
  }
  
  log('[spfDmarc] Completed email security scan, found', findingsCount, 'issues');
  return findingsCount;
}
</file>

<file path="modules/spiderFoot.ts">
/*
 * =============================================================================
 * MODULE: spiderFoot.ts (Refactored)
 * =============================================================================
 * This module is a robust wrapper for the SpiderFoot OSINT tool.
 *
 * Key Improvements from previous version:
 * 1.  **Advanced Protocol Probing:** When an INTERNET_NAME (domain) is found,
 * this module now actively probes for both http:// and https:// and performs
 * an advanced health check, verifying a `200 OK` status before creating a
 * URL artifact. This improves the accuracy of downstream tools.
 * 2.  **API Key Dependency Warnings:** The module now checks for critical API
 * keys at startup. If keys are missing, it creates a `scan_warning` artifact
 * to make the potentially incomplete results visible in the scan output.
 * =============================================================================
 */

import { execFile, exec as execRaw } from 'node:child_process';
import { promisify } from 'node:util';
import fs from 'node:fs/promises';
import axios from 'axios';
import { insertArtifact } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const execFileAsync = promisify(execFile);
const execAsync = promisify(execRaw);

const ALLOW_SET = new Set<string>([
  'DOMAIN_NAME', 'INTERNET_DOMAIN', 'SUBDOMAIN', 'INTERNET_NAME', 'CO_HOSTED_SITE',
  'NETBLOCK_OWNER', 'RAW_RIR_DATA', 'AFFILIATE_INTERNET_NAME', 'IP_ADDRESS',
  'EMAILADDR', 'VULNERABILITY_CVE', 'MALICIOUS_IPADDR', 'MALICIOUS_INTERNET_NAME',
  'LEAKSITE_CONTENT', 'PASTESITE_CONTENT',
]);
const DENY_SET = new Set<string>();

function shouldPersist(rowType: string): boolean {
  const mode = (process.env.SPIDERFOOT_FILTER_MODE || 'allow').toLowerCase();
  switch (mode) {
    case 'off': return true;
    case 'deny': return !DENY_SET.has(rowType);
    case 'allow': default: return ALLOW_SET.has(rowType);
  }
}

/**
 * REFACTOR: Implemented advanced health checks. Now uses a GET request and
 * verifies a 200 OK status for more reliable endpoint validation.
 */
async function probeAndCreateUrlArtifacts(domain: string, baseArtifact: any): Promise<number> {
    const protocols = ['https', 'http'];
    let urlsCreated = 0;
    for (const proto of protocols) {
        const url = `${proto}://${domain}`;
        try {
            const response = await axios.get(url, { 
                timeout: 8000,
                headers: { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36' }
            });

            // Check for a definitive "OK" status. This is more reliable than just not erroring.
            if (response.status === 200) {
                await insertArtifact({ ...baseArtifact, type: 'url', val_text: url });
                urlsCreated++;
            }
        } catch (error) {
            // Ignore connection errors, 404s, 5xx, etc.
        }
    }
    return urlsCreated;
}

const TARGET_MODULES = [
  'sfp_crtsh', 'sfp_censys', 'sfp_sublist3r', 'sfp_shodan', 'sfp_chaos',
  'sfp_r7_dns', 'sfp_haveibeenpwnd', 'sfp_psbdmp', 'sfp_skymem',
  'sfp_sslcert', 'sfp_nuclei', 'sfp_whois', 'sfp_dnsresolve',
].join(',');

async function resolveSpiderFootCommand(): Promise<string | null> {
    if (process.env.SPIDERFOOT_CMD) return process.env.SPIDERFOOT_CMD;
    const candidates = [
        '/opt/spiderfoot/sf.py', '/usr/local/bin/sf', 'sf', 'spiderfoot.py',
    ];
    for (const cand of candidates) {
        try {
            if (cand.startsWith('/')) {
                await fs.access(cand, fs.constants.X_OK);
                return cand.includes('.py') ? `python3 ${cand}` : cand;
            }
            await execFileAsync('which', [cand]);
            return cand;
        } catch { /* next */ }
    }
    return null;
}

export async function runSpiderFoot(job: { domain: string; scanId: string }): Promise<number> {
    const { domain, scanId } = job;
    log(`[SpiderFoot] Starting scan for ${domain} (scanId=${scanId})`);

    const spiderFootCmd = await resolveSpiderFootCommand();
    if (!spiderFootCmd) {
        log('[SpiderFoot] [CRITICAL] Binary not found  module skipped');
        await insertArtifact({
            type: 'scan_error',
            val_text: 'SpiderFoot binary not found in container',
            severity: 'HIGH',
            meta: { scan_id: scanId, module: 'spiderfoot' },
        });
        return 0;
    }

    const confDir = `/tmp/spiderfoot-${scanId}`;
    await fs.mkdir(confDir, { recursive: true });

    const config = {
        shodan_api_key: process.env.SHODAN_API_KEY ?? '',
        censys_api_key: process.env.CENSYS_API_KEY ?? '',
        haveibeenpwnd_api_key: process.env.HIBP_API_KEY ?? '',
        chaos_api_key: process.env.CHAOS_API_KEY ?? '',
        dbconnectstr: `sqlite:////tmp/spiderfoot-${scanId}.db`,
        webport: '5001',
        webhost: '127.0.0.1',
    };
    
    const missingKeys = Object.entries(config)
        .filter(([key, value]) => key.endsWith('_api_key') && !value)
        .map(([key]) => key);
    
    if (missingKeys.length > 0) {
        const warningText = `SpiderFoot scan may be incomplete. Missing API keys: ${missingKeys.join(', ')}`;
        log(`[SpiderFoot] [WARNING] ${warningText}`);
        await insertArtifact({
            type: 'scan_warning',
            val_text: warningText,
            severity: 'LOW',
            meta: { scan_id: scanId, module: 'spiderfoot', missing_keys: missingKeys }
        });
    }

    const mask = (v: string) => (v ? '' : '');
    log(`[SpiderFoot] API keys: Shodan ${mask(config.shodan_api_key)}, Censys ${mask(config.censys_api_key)}, HIBP ${mask(config.haveibeenpwnd_api_key)}, Chaos ${mask(config.chaos_api_key)}`);
    await fs.writeFile(`${confDir}/spiderfoot.conf`, Object.entries(config).map(([k, v]) => `${k}=${v}`).join('\n'));
    
    const cmd = `${spiderFootCmd} -q -s ${domain} -m ${TARGET_MODULES} -o json`;
    log('[SpiderFoot] Command:', cmd);
    
    const env = { ...process.env, SF_CONFDIR: confDir };
    const TIMEOUT_MS = parseInt(process.env.SPIDERFOOT_TIMEOUT_MS || '300000', 10);
    
    try {
        const start = Date.now();
        const { stdout, stderr } = await execAsync(cmd, { env, timeout: TIMEOUT_MS, shell: '/bin/sh', maxBuffer: 20 * 1024 * 1024 });
        if (stderr) log('[SpiderFoot-stderr]', stderr.slice(0, 400));
        log(`[SpiderFoot] Raw output size: ${stdout.length} bytes`);

        const results = stdout.trim() ? JSON.parse(stdout) : [];
        let artifacts = 0;
        
        for (const row of results) {
            if (!shouldPersist(row.type)) continue;

            const base = {
                severity: /VULNERABILITY|MALICIOUS/.test(row.type) ? 'HIGH' : 'INFO',
                src_url: row.sourceUrl ?? domain,
                meta: { scan_id: scanId, spiderfoot_type: row.type, source_module: row.module },
            } as const;
            
            let created = false;
            switch (row.type) {
                case 'AFFILIATE_INTERNET_NAME':
                case 'INTERNET_NAME': {
                    await insertArtifact({ ...base, type: 'subdomain', val_text: row.data });
                    const urlsCreated = await probeAndCreateUrlArtifacts(row.data, base);
                    artifacts += (1 + urlsCreated);
                    continue;
                }
                default:
                    await insertArtifact({ ...base, type: 'intel', val_text: row.data });
                    created = true;
            }
            if (created) artifacts++;
        }
        
        await insertArtifact({
            type: 'scan_summary',
            val_text: `SpiderFoot scan completed: ${artifacts} artifacts`,
            severity: 'INFO',
            meta: { scan_id: scanId, duration_ms: Date.now() - start, results_processed: results.length, artifacts_created: artifacts, timestamp: new Date().toISOString() },
        });
        
        log(`[SpiderFoot]  Completed  ${artifacts} artifacts`);
        return artifacts;
    } catch (err: any) {
        log('[SpiderFoot]  Scan failed:', err.message);
        await insertArtifact({
            type: 'scan_error',
            val_text: `SpiderFoot scan failed: ${err.message}`,
            severity: 'HIGH',
            meta: { scan_id: scanId, module: 'spiderfoot' },
        });
        return 0;
    }
}
</file>

<file path="modules/tlsScan.ts">
/*
 * =============================================================================
 * MODULE: tlsScan.ts (Refactored)
 * =============================================================================
 * This module performs a deep TLS/SSL security configuration analysis using the
 * 'testssl.sh' script.
 *
 * Key Improvements from previous version:
 * 1.  **Reliable JSON Parsing:** The module now uses the JSON output from
 * testssl.sh instead of fragile stdout text parsing. This ensures all
 * findings are accurately captured.
 * 2.  **Precise Certificate Expiry:** The certificate check is no longer based on
 * a simple year match. It now calculates the exact days remaining until
 * expiration and creates tiered-severity findings based on urgency.
 * 3.  **Comprehensive Vulnerability Mapping:** It iterates through all findings
 * in the JSON report, creating detailed artifacts for each issue based on
 * its ID and severity, covering everything from weak protocols to specific
 * CVEs like Heartbleed.
 * =============================================================================
 */

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import fs from 'node:fs/promises';
import { insertArtifact, insertFinding } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const exec = promisify(execFile);

// Represents a single finding from the testssl.sh JSON output.
interface TlsFinding {
    id: string;
    ip: string;
    port: string;
    severity: 'OK' | 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' | 'INFO';
    finding: string;
}

const TLS_SCAN_TIMEOUT_MS = parseInt(process.env.TLS_SCAN_TIMEOUT_MS || '300000'); // 5 minutes default

/**
 * Resolves the testssl.sh binary path, similar to SpiderFoot resolver
 */
async function resolveTestsslPath(): Promise<string> {
    const possiblePaths = [
        '/opt/testssl.sh/testssl.sh',
        '/usr/local/bin/testssl.sh',
        '/usr/bin/testssl.sh',
        'testssl.sh' // In PATH
    ];

    for (const path of possiblePaths) {
        try {
            await exec(path, ['--version'], { timeout: 5000 });
            log(`[tlsScan] Found testssl.sh at: ${path}`);
            return path;
        } catch {
            // Continue to next path
        }
    }
    
    throw new Error('testssl.sh not found in any expected location');
}

export async function runTlsScan(job: { domain: string; scanId?: string }): Promise<number> {
    log('[tlsScan] Starting TLS/SSL security scan for', job.domain);
    const jsonOutputFile = `/tmp/testssl_${job.scanId || job.domain}.json`;
    let findingsCount = 0;

    try {
        const testsslPath = await resolveTestsslPath();
        
        await exec(testsslPath, [
            '--quiet',
            '--warnings', 'off',
            '--jsonfile', jsonOutputFile,
            job.domain
        ], {
            timeout: TLS_SCAN_TIMEOUT_MS
        });

        const reportData = await fs.readFile(jsonOutputFile, 'utf-8');
        const report = JSON.parse(reportData);
        
        // --- 1. Process all findings from the report with proper structure handling ---
        // testssl.sh JSON structure: scanResult is array of test objects with .id, .severity, .finding
        const findings: TlsFinding[] = (report.scanResult ?? report.findings ?? []) as TlsFinding[];
        
        log(`[tlsScan] Processing ${findings.length} findings from testssl.sh report`);
        
        for (const finding of findings) {
            // We only care about actionable findings, not "OK" or "INFO" statuses.
            // Note: "LOW" severity findings are intentionally included as they may be actionable
            if (finding.severity === 'OK' || finding.severity === 'INFO') {
                continue;
            }

            findingsCount++;
            const artifactId = await insertArtifact({
                type: 'tls_weakness',
                val_text: `${job.domain} - ${finding.id}: ${finding.finding}`,
                severity: finding.severity,
                meta: {
                    finding_id: finding.id,
                    details: finding.finding,
                    scan_id: job.scanId,
                    scan_module: 'tlsScan'
                }
            });
            
            await insertFinding(
                artifactId,
                'TLS_CONFIGURATION_ISSUE',
                getTlsRecommendation(finding.id),
                finding.finding
            );
        }
        
        // --- 2. Perform precise certificate expiration check ---
        // Check both serverDefaults and scanResult for certificate info
        let certInfo = report.serverDefaults?.cert_notAfter || 
                      report.scanResult?.find((r: any) => r.id === 'cert_notAfter')?.finding;
        
        if (certInfo) {
            try {
                const expiryDate = new Date(certInfo);
                const today = new Date();
                const daysUntilExpiry = Math.ceil((expiryDate.getTime() - today.getTime()) / (1000 * 3600 * 24));
                
                let severity: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL' | null = null;
                let recommendation = '';

                if (daysUntilExpiry <= 0) {
                    severity = 'CRITICAL';
                    recommendation = `Certificate expired ${Math.abs(daysUntilExpiry)} days ago. Renew immediately to prevent service disruption and security warnings.`;
                } else if (daysUntilExpiry <= 14) {
                    severity = 'HIGH';
                    recommendation = `Certificate expires in ${daysUntilExpiry} days. Renew immediately.`;
                } else if (daysUntilExpiry <= 30) {
                    severity = 'MEDIUM';
                    recommendation = `Certificate expires in ${daysUntilExpiry} days. Plan renewal soon.`;
                } else if (daysUntilExpiry <= 90) {
                    severity = 'LOW';
                    recommendation = `Certificate expires in ${daysUntilExpiry} days. No immediate action needed, but be aware of the upcoming renewal.`;
                }

                if (severity) {
                    findingsCount++;
                    const artifactId = await insertArtifact({
                        type: 'tls_certificate_expiry',
                        val_text: `${job.domain} - SSL certificate expires in ${daysUntilExpiry} days`,
                        severity: severity,
                        meta: {
                            expiry_date: expiryDate.toISOString(),
                            days_remaining: daysUntilExpiry,
                            scan_id: job.scanId,
                            scan_module: 'tlsScan'
                        }
                    });
                    await insertFinding(artifactId, 'CERTIFICATE_EXPIRY', recommendation, `The SSL/TLS certificate for ${job.domain} is nearing its expiration date.`);
                }
            } catch (dateError) {
                log(`[tlsScan] Warning: Could not parse certificate expiry date: ${certInfo}`);
            }
        } else {
            // No certificate found - create explicit artifact
            log(`[tlsScan] No certificate information found for ${job.domain}`);
            findingsCount++;
            const artifactId = await insertArtifact({
                type: 'tls_no_certificate',
                val_text: `${job.domain} - No SSL/TLS certificate found`,
                severity: 'HIGH',
                meta: {
                    scan_id: job.scanId,
                    scan_module: 'tlsScan',
                    reason: 'No certificate presented by server'
                }
            });
            await insertFinding(
                artifactId, 
                'MISSING_TLS_CERTIFICATE', 
                'Configure SSL/TLS certificate for this domain to enable encrypted connections',
                'The server did not present any SSL/TLS certificate'
            );
        }

        log(`[tlsScan] Completed TLS scan, found ${findingsCount} issues`);

    } catch (error) {
        log('[tlsScan] [ERROR] Error during scan:', (error as Error).message);
        await insertArtifact({
            type: 'scan_error',
            val_text: 'TLS scan failed to execute.',
            severity: 'HIGH',
            meta: {
                scan_id: job.scanId,
                scan_module: 'tlsScan',
                error: (error as Error).message
            }
        });
    } finally {
        // Ensure the temporary JSON file is cleaned up.
        await fs.unlink(jsonOutputFile).catch(err => {
            log(`[tlsScan] [WARNING] Failed to delete temp file: ${jsonOutputFile}`, err.message);
        });
    }
    
    // Add completion tracking artifact regardless of success or failure.
    await insertArtifact({
        type: 'scan_summary',
        val_text: `TLS scan completed: ${findingsCount} issues found`,
        severity: 'INFO',
        meta: {
            scan_id: job.scanId,
            scan_module: 'tlsScan',
            total_findings: findingsCount,
            timestamp: new Date().toISOString()
        }
    });

    return findingsCount;
}

/**
 * Maps testssl.sh test IDs to specific recommendations
 */
function getTlsRecommendation(testId: string): string {
    const recommendations: Record<string, string> = {
        'cert_chain': 'Fix certificate chain by ensuring proper intermediate certificates are included',
        'cert_commonName': 'Ensure certificate Common Name or SAN matches the requested domain',
        'protocols': 'Disable weak protocols (SSL 2.0, SSL 3.0, TLS 1.0, TLS 1.1) and use TLS 1.2+',
        'ciphers': 'Disable weak ciphers and cipher suites, use strong AEAD ciphers',
        'pfs': 'Enable Perfect Forward Secrecy by configuring ECDHE cipher suites',
        'rc4': 'Disable RC4 cipher completely due to known weaknesses',
        'heartbleed': 'Update OpenSSL to version 1.0.1g or later to fix Heartbleed vulnerability',
        'ccs': 'Update OpenSSL to fix CCS Injection vulnerability (CVE-2014-0224)',
        'secure_renegotiation': 'Enable secure renegotiation to prevent renegotiation attacks',
        'crime': 'Disable TLS compression to prevent CRIME attacks',
        'breach': 'Disable HTTP compression or implement proper CSRF protection for BREACH mitigation'
    };

    // Find matching recommendation
    for (const [key, recommendation] of Object.entries(recommendations)) {
        if (testId.toLowerCase().includes(key.toLowerCase())) {
            return recommendation;
        }
    }

    return 'Review TLS configuration and apply security best practices according to current standards';
}
</file>

<file path="modules/trufflehog.ts">
/*
 * =============================================================================
 * MODULE: trufflehog.ts (Refactored)
 * =============================================================================
 * This module runs TruffleHog to find secrets in Git repositories, websites,
 * and local files from other scan modules.
 *
 * Key Improvements from previous version:
 * 1.  **Hardened Website Crawler:** The crawler now includes resource limits
 * (file size, total files, total size) and secure filename sanitization to
 * prevent resource exhaustion and path traversal attacks.
 * 2.  **Expanded Git Repo Scanning:** The limit on the number of GitHub repos
 * scanned has been increased for better coverage.
 * 3.  **Targeted File Scanning:** Overly broad filesystem globs have been replaced
 * with more specific patterns that target the known output files from other
 * modules like spiderFoot and documentExposure.
 * =============================================================================
 */

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import fs from 'node:fs/promises';
import path from 'node:path';
import axios from 'axios';
import { parse } from 'node-html-parser';
import { insertArtifact } from '../core/artifactStore.js';
import { log } from '../core/logger.js';

const exec = promisify(execFile);
const GITHUB_RE = /^https:\/\/github\.com\/([\w.-]+\/[\w.-]+)(\.git)?$/i;
const MAX_CRAWL_DEPTH = 2;
const MAX_GIT_REPOS_TO_SCAN = 20;
const TRUFFLEHOG_GIT_DEPTH = parseInt(process.env.TRUFFLEHOG_GIT_DEPTH || '5'); // Reduced default depth

// REFACTOR: Added resource limits for the website crawler.
const MAX_FILE_SIZE_BYTES = 5 * 1024 * 1024; // 5MB per file
const MAX_FILES_PER_CRAWL = 50; // Max 50 files per domain
const MAX_TOTAL_CRAWL_SIZE_BYTES = 50 * 1024 * 1024; // 50MB total
const MAX_PAGES = 250; // Maximum pages to crawl to prevent deep link farm attacks

/**
 * Processes the JSON line-by-line output from a TruffleHog scan.
 */
async function processTrufflehogOutput(stdout: string, source_type: 'git' | 'http' | 'file', src_url: string): Promise<number> {
    const lines = stdout.trim().split('\n').filter(Boolean);
    let findings = 0;

    for (const line of lines) {
        try {
            const obj = JSON.parse(line);
            findings++;
            await insertArtifact({
                type: 'secret',
                val_text: `${obj.DetectorName}: ${obj.Raw.slice(0, 50)}`,
                severity: obj.Verified ? 'CRITICAL' : 'HIGH',
                src_url: src_url,
                meta: {
                    detector: obj.DetectorName,
                    verified: obj.Verified,
                    source_type: source_type,
                    file: obj.SourceMetadata?.Data?.Filesystem?.file ?? 'N/A',
                    line: obj.SourceMetadata?.Data?.Filesystem?.line ?? 0
                }
            });
        } catch (e) {
            log('[trufflehog] [ERROR] Failed to parse JSON output line:', (e as Error).message);
        }
    }
    return findings;
}


async function scanGit(url: string): Promise<number> {
    log('[trufflehog] [Git Scan] Starting scan for repository:', url);
    try {
        const { stdout } = await exec('trufflehog', [
            'git', 
            url, 
            '--json', 
            '--no-verification', 
            `--max-depth=${TRUFFLEHOG_GIT_DEPTH}`
        ], { maxBuffer: 20 * 1024 * 1024 });
        return await processTrufflehogOutput(stdout, 'git', url);
    } catch (err) {
        log('[trufflehog] [Git Scan] Error scanning repository', url, (err as Error).message);
        return 0;
    }
}

/**
 * REFACTOR: Hardened the crawler with resource limits and secure filename sanitization.
 * Now includes protection against deep link farms.
 */
async function scanWebsite(domain: string, scanId: string): Promise<number> {
    log('[trufflehog] [Website Scan] Starting crawl and scan for:', domain);
    const baseUrl = `https://${domain}`;
    const scanDir = `/tmp/trufflehog_crawl_${scanId}`;
    const visited = new Set<string>();
    let filesWritten = 0;
    let totalDownloadedSize = 0;
    let pagesVisited = 0; // Track total pages to prevent link farm attacks

    try {
        await fs.mkdir(scanDir, { recursive: true });

        const crawl = async (url: string, depth: number) => {
            // Check resource limits before proceeding - now includes page count limit
            if (depth > MAX_CRAWL_DEPTH || 
                visited.has(url) || 
                filesWritten >= MAX_FILES_PER_CRAWL || 
                totalDownloadedSize >= MAX_TOTAL_CRAWL_SIZE_BYTES ||
                pagesVisited >= MAX_PAGES) {
                return;
            }
            visited.add(url);
            pagesVisited++;

            try {
                const response = await axios.get(url, {
                    timeout: 10000,
                    maxContentLength: MAX_FILE_SIZE_BYTES,
                    maxBodyLength: MAX_FILE_SIZE_BYTES,
                });
                
                totalDownloadedSize += response.data.length;
                filesWritten++;

                // REFACTOR: Implemented secure filename sanitization.
                const safeName = (path.basename(new URL(url).pathname) || 'index.html').replace(/[^a-zA-Z0-9.-]/g, '_');
                const filePath = path.join(scanDir, safeName);

                await fs.writeFile(filePath, response.data);
                
                const contentType = response.headers['content-type'] || '';
                if (contentType.includes('text/html')) {
                    const root = parse(response.data);
                    const links = root.querySelectorAll('a[href], script[src]');
                    for (const link of links) {
                        const href = link.getAttribute('href') || link.getAttribute('src');
                        if (href) {
                            try {
                                const absoluteUrl = new URL(href, baseUrl).toString();
                                if (absoluteUrl.startsWith(baseUrl)) {
                                    await crawl(absoluteUrl, depth + 1);
                                }
                            } catch { /* Ignore malformed URLs */ }
                        }
                    }
                }
            } catch (crawlError) {
                log(`[trufflehog] [Website Scan] Failed to crawl or download ${url}:`, (crawlError as Error).message);
            }
        };

        await crawl(baseUrl, 1);

        if (filesWritten > 0) {
            log(`[trufflehog] [Website Scan] Crawl complete. Scanned ${pagesVisited} pages, downloaded ${filesWritten} files.`);
            const { stdout } = await exec('trufflehog', ['filesystem', scanDir, '--json', '--no-verification'], { maxBuffer: 20 * 1024 * 1024 });
            return await processTrufflehogOutput(stdout, 'http', baseUrl);
        }
        return 0;

    } catch (err) {
        log('[trufflehog] [Website Scan] An unexpected error occurred:', (err as Error).message);
        return 0;
    } finally {
        await fs.rm(scanDir, { recursive: true, force: true }).catch(() => {});
    }
}


/**
 * REFACTOR: Replaces overly broad glob patterns with more targeted paths based
 * on the known outputs of other scanner modules.
 */
async function scanLocalFiles(scanId: string): Promise<number> {
    log('[trufflehog] [File Scan] Scanning local artifacts...');
    const filePathsToScan = [
        `/tmp/spiderfoot-links-${scanId}.json`, // SpiderFoot link list
        // Add paths to other known module outputs here if necessary.
    ];
    let findings = 0;

    for (const filePath of filePathsToScan) {
        try {
            await fs.access(filePath);
            const { stdout } = await exec('trufflehog', ['filesystem', filePath, '--json', '--no-verification'], { maxBuffer: 10 * 1024 * 1024 });
            findings += await processTrufflehogOutput(stdout, 'file', `local:${filePath}`);
        } catch (error) {
            // This is expected if a file doesn't exist, so no noisy log needed.
        }
    }
    return findings;
}


export async function runTrufflehog(job: { domain: string; scanId?: string }): Promise<number> {
  log('[trufflehog] Starting secret scan for domain:', job.domain);
  if (!job.scanId) {
      log('[trufflehog] [ERROR] scanId is required for TruffleHog module.');
      return 0;
  }
  let totalFindings = 0;

  totalFindings += await scanWebsite(job.domain, job.scanId);

  try {
    const linksPath = `/tmp/spiderfoot-links-${job.scanId}.json`;
    const linksFile = await fs.readFile(linksPath, 'utf8');
    const links = JSON.parse(linksFile) as string[];
    const gitRepos = links.filter(l => GITHUB_RE.test(l)).slice(0, MAX_GIT_REPOS_TO_SCAN);
    
    log(`[trufflehog] Found ${gitRepos.length} GitHub repositories to scan.`);
    for (const repo of gitRepos) {
      totalFindings += await scanGit(repo);
    }
  } catch {
    log('[trufflehog] No SpiderFoot links file found, or unable to parse. Skipping Git repo scan.');
  }

  totalFindings += await scanLocalFiles(job.scanId);

  log('[trufflehog] Finished secret scan for', job.domain, 'Total secrets found:', totalFindings);
  
  await insertArtifact({
    type: 'scan_summary',
    val_text: `TruffleHog scan completed: ${totalFindings} potential secrets found`,
    severity: 'INFO',
    meta: {
      scan_id: job.scanId,
      scan_module: 'trufflehog',
      total_findings: totalFindings,
      timestamp: new Date().toISOString()
    }
  });
  
  return totalFindings;
}
</file>

<file path="templates/dorks-optimized.txt">
# Optimized Google Dorks - Reduced from 94 to 22 queries
# Each line represents a consolidated query that maintains full coverage

# Domain file discovery (1 query instead of 17)
site:DOMAIN (filetype:pdf OR filetype:doc OR filetype:docx OR filetype:xls OR filetype:xlsx OR filetype:ppt OR filetype:pptx OR filetype:txt OR filetype:csv OR filetype:sql OR filetype:log OR filetype:zip OR filetype:tar OR filetype:gz OR filetype:backup OR filetype:bak OR filetype:old)

# Company document search (1 query instead of 5)
COMPANY_NAME (filetype:pdf OR filetype:doc OR filetype:docx OR filetype:xls OR filetype:xlsx)

# Sensitive PDF documents (1 query instead of 11)
COMPANY_NAME ("confidential" OR "internal" OR "private" OR "financial" OR "budget" OR "salary" OR "contract" OR "agreement" OR "employee" OR "org chart" OR "organization chart") filetype:pdf

# Database files (1 query instead of 3)
COMPANY_NAME ("database" OR "backup" OR "dump") filetype:sql

# Configuration and secrets in text files (1 query instead of 8)
COMPANY_NAME ("config" OR "configuration" OR "password" OR "passwords" OR "credentials" OR "api key" OR "secret" OR "token") filetype:txt

# Code repositories (1 query instead of 7)
COMPANY_NAME (site:github.com OR site:gitlab.com OR site:bitbucket.org OR site:pastebin.com OR site:paste.ee OR site:justpaste.it OR site:rentry.co)

# Exposed configuration files in URLs (1 query instead of 6)
COMPANY_NAME (inurl:"wp-config.php.txt" OR inurl:".env" OR inurl:"config.php" OR inurl:"settings.php" OR inurl:"database.yml" OR inurl:"credentials.json" OR inurl:"secrets.yml")

# Directory listings (1 query instead of 2)
COMPANY_NAME (intitle:"index of" OR intitle:"directory listing")

# Database connection strings (1 query instead of 10)
COMPANY_NAME (intext:"mysql_connect" OR intext:"mysql_pconnect" OR intext:"pg_connect" OR intext:"mssql_connect" OR intext:"oracle_connect" OR intext:"mongodb://" OR intext:"postgres://" OR intext:"redis://" OR intext:"ftp://" OR intext:"sftp://")

# Configuration files by extension (1 query instead of 8)
COMPANY_NAME (ext:env OR ext:ini OR ext:cfg OR ext:conf OR ext:config OR ext:properties OR ext:yaml OR ext:yml)

# JSON files with secrets (1 query instead of 4)
COMPANY_NAME ext:json ("password" OR "secret" OR "key" OR "token")

# XML files with secrets (1 query instead of 2)
COMPANY_NAME ext:xml ("password" OR "secret")

# HubSpot CDN general search (1 query instead of 2)
COMPANY_NAME (site:*.hubspotusercontent*.net OR site:*.hs-sites.com)

# HubSpot specific file search
site:*.hubspotusercontent*.net inurl:/hubfs COMPANY_NAME

# Salesforce CDN general search (1 query instead of 4)
COMPANY_NAME (site:*.my.salesforce.com OR site:*.content.force.com OR site:*.visualforce.com OR site:*.lightning.force.com)

# Salesforce file downloads
site:*.my.salesforce.com inurl:"/servlet/servlet.FileDownload" COMPANY_NAME

# Salesforce document shepherd
site:*.content.force.com inurl:"/sfc/servlet.shepherd/document" COMPANY_NAME

# HubSpot sensitive PDFs
COMPANY_NAME site:*.hubspotusercontent*.net filetype:pdf

# Salesforce sensitive spreadsheets
COMPANY_NAME site:*.salesforce.com filetype:xlsx

# Force.com confidential content
COMPANY_NAME site:*.force.com "confidential"

# HubSpot internal documents
COMPANY_NAME site:*.hubspot*.net "internal use only"

# --- NEW PLATFORMS ---

# Google Drive / Docs
(site:drive.google.com/file/d/ OR site:docs.google.com/document/d/ OR site:drive.google.com/drive/folders/) COMPANY_NAME ("confidential" OR "internal" OR "proprietary")

# Microsoft SharePoint/OneDrive
(site:*.sharepoint.com OR site:onedrive.live.com/redir?resid=) COMPANY_NAME (filetype:xlsx OR filetype:docx) ("financial report" OR "confidential" OR "proprietary")

# Box
site:app.box.com/s/ COMPANY_NAME ("internal documents" OR "sensitive data")

# Dropbox
site:dropbox.com/s/ COMPANY_NAME ("sensitive data" OR "internal plans")

# --- API DOCUMENTATION ---
COMPANY_NAME (filetype:yaml OR filetype:json) (intext:"swagger" OR intext:"openapi" OR intext:"API documentation")
</file>

<file path="templates/dorks.txt">
site:DOMAIN filetype:pdf
site:DOMAIN filetype:doc
site:DOMAIN filetype:docx  
site:DOMAIN filetype:xls
site:DOMAIN filetype:xlsx
site:DOMAIN filetype:ppt
site:DOMAIN filetype:pptx
site:DOMAIN filetype:txt
site:DOMAIN filetype:csv
site:DOMAIN filetype:sql
site:DOMAIN filetype:log
site:DOMAIN filetype:zip
site:DOMAIN filetype:tar
site:DOMAIN filetype:gz
site:DOMAIN filetype:backup
site:DOMAIN filetype:bak
site:DOMAIN filetype:old
COMPANY_NAME filetype:pdf
COMPANY_NAME filetype:doc
COMPANY_NAME filetype:docx
COMPANY_NAME filetype:xls
COMPANY_NAME filetype:xlsx
COMPANY_NAME "confidential" filetype:pdf
COMPANY_NAME "internal" filetype:pdf
COMPANY_NAME "private" filetype:pdf
COMPANY_NAME "financial" filetype:pdf
COMPANY_NAME "budget" filetype:pdf
COMPANY_NAME "salary" filetype:pdf
COMPANY_NAME "contract" filetype:pdf
COMPANY_NAME "agreement" filetype:pdf
COMPANY_NAME "employee" filetype:pdf
COMPANY_NAME "org chart" filetype:pdf
COMPANY_NAME "organization chart" filetype:pdf
COMPANY_NAME "database" filetype:sql
COMPANY_NAME "backup" filetype:sql
COMPANY_NAME "dump" filetype:sql
COMPANY_NAME "config" filetype:txt
COMPANY_NAME "configuration" filetype:txt
COMPANY_NAME "password" filetype:txt
COMPANY_NAME "passwords" filetype:txt
COMPANY_NAME "credentials" filetype:txt
COMPANY_NAME "api key" filetype:txt
COMPANY_NAME "secret" filetype:txt
COMPANY_NAME "token" filetype:txt
site:github.com COMPANY_NAME
site:gitlab.com COMPANY_NAME
site:bitbucket.org COMPANY_NAME
site:pastebin.com COMPANY_NAME
site:paste.ee COMPANY_NAME
site:justpaste.it COMPANY_NAME
site:rentry.co COMPANY_NAME
inurl:"wp-config.php.txt" COMPANY_NAME
inurl:".env" COMPANY_NAME
inurl:"config.php" COMPANY_NAME
inurl:"settings.php" COMPANY_NAME
inurl:"database.yml" COMPANY_NAME
inurl:"credentials.json" COMPANY_NAME
inurl:"secrets.yml" COMPANY_NAME
intitle:"index of" COMPANY_NAME
intitle:"directory listing" COMPANY_NAME
COMPANY_NAME intext:"mysql_connect"
COMPANY_NAME intext:"mysql_pconnect"
COMPANY_NAME intext:"pg_connect"
COMPANY_NAME intext:"mssql_connect"
COMPANY_NAME intext:"oracle_connect"
COMPANY_NAME intext:"mongodb://"
COMPANY_NAME intext:"postgres://"
COMPANY_NAME intext:"redis://"
COMPANY_NAME intext:"ftp://"
COMPANY_NAME intext:"sftp://"
COMPANY_NAME ext:env
COMPANY_NAME ext:ini
COMPANY_NAME ext:cfg
COMPANY_NAME ext:conf
COMPANY_NAME ext:config
COMPANY_NAME ext:properties
COMPANY_NAME ext:yaml
COMPANY_NAME ext:yml
COMPANY_NAME ext:json "password"
COMPANY_NAME ext:json "secret"
COMPANY_NAME ext:json "key"
COMPANY_NAME ext:json "token"
COMPANY_NAME ext:xml "password"
COMPANY_NAME ext:xml "secret"
site:*.hubspotusercontent*.net inurl:/hubfs COMPANY_NAME
site:*.hs-sites.com COMPANY_NAME
site:*.my.salesforce.com inurl:"/servlet/servlet.FileDownload" COMPANY_NAME
site:*.content.force.com inurl:"/sfc/servlet.shepherd/document" COMPANY_NAME
site:*.visualforce.com COMPANY_NAME
site:*.lightning.force.com COMPANY_NAME
COMPANY_NAME site:*.hubspotusercontent*.net filetype:pdf
COMPANY_NAME site:*.salesforce.com filetype:xlsx
COMPANY_NAME site:*.force.com "confidential"
COMPANY_NAME site:*.hubspot*.net "internal use only"
</file>

<file path="templates/nuclei-custom.yaml">
id: supabase-unauthenticated-access

info:
  name: Supabase Unauthenticated Access
  author: dealbrief-scanner
  severity: high
  description: Detects unauthenticated access to Supabase endpoints
  tags: supabase,database,unauth

http:
  - method: GET
    path:
      - "{{BaseURL}}/rest/v1/"
      - "{{BaseURL}}/auth/v1/"
      - "{{BaseURL}}/storage/v1/"

    matchers-condition: and
    matchers:
      - type: word
        words:
          - "supabase"
          - "postgrest"
        condition: or
      - type: status
        status:
          - 200
          - 401

---

id: neon-database-exposure

info:
  name: Neon Database Connection Exposure
  author: dealbrief-scanner
  severity: critical
  description: Detects exposed Neon database connection strings
  tags: neon,database,exposure

http:
  - method: GET
    path:
      - "{{BaseURL}}/.env"
      - "{{BaseURL}}/config.json"
      - "{{BaseURL}}/config.js"

    matchers:
      - type: regex
        regex:
          - "postgresql://.*@.*\\.neon\\.tech/.*"
          - "DATABASE_URL.*neon\\.tech"

---

id: s3-bucket-misconfiguration

info:
  name: S3 Bucket Misconfiguration
  author: dealbrief-scanner
  severity: high
  description: Detects S3 bucket misconfigurations
  tags: s3,aws,bucket,misconfiguration

http:
  - method: GET
    path:
      - "{{BaseURL}}"

    matchers:
      - type: word
        words:
          - "ListBucketResult"
          - "Key"
          - "LastModified"
        condition: and

---

id: gcs-bucket-exposure

info:
  name: Google Cloud Storage Bucket Exposure
  author: dealbrief-scanner
  severity: high
  description: Detects exposed GCS buckets
  tags: gcs,google,bucket,exposure

http:
  - method: GET
    path:
      - "{{BaseURL}}"

    matchers:
      - type: word
        words:
          - "storage.googleapis.com"
          - "ListBucketResult"
        condition: and

---

id: exposed-admin-panels-refined

info:
  name: Exposed Admin Login Panels (Refined)
  author: dealbrief-scanner
  severity: medium
  description: Detects known admin login panels based on specific title tags and form fields.
  tags: admin,panel,exposure,login

http:
  - method: GET
    path:
      - "{{BaseURL}}/wp-login.php"
      - "{{BaseURL}}/administrator/index.php"
      - "{{BaseURL}}/admin/login"

    matchers-condition: or
    matchers:
      # WordPress
      - type: word
        part: body
        words:
          - "<title>Log In &lsaquo; WordPress</title>"
          - 'name="log"'
          - 'name="pwd"'
        condition: and
      # Joomla
      - type: word
        part: body
        words:
          - '<title>Joomla! Administrator</title>'
          - 'name="username"'
          - 'name="passwd"'
        condition: and
      # Generic login indicators
      - type: word
        part: body
        words:
          - '<title>Admin</title>'
          - 'name="username"'
          - 'name="password"'
        condition: and

---

id: exposed-env-files

info:
  name: Exposed Environment Files
  author: dealbrief-scanner
  severity: high
  description: Detects exposed environment configuration files
  tags: env,config,exposure

http:
  - method: GET
    path:
      - "{{BaseURL}}/.env"
      - "{{BaseURL}}/.env.local"
      - "{{BaseURL}}/.env.production"
      - "{{BaseURL}}/.env.development"
      - "{{BaseURL}}/config.json"
      - "{{BaseURL}}/config.yaml"
      - "{{BaseURL}}/config.yml"

    matchers:
      - type: word
        words:
          - "API_KEY"
          - "SECRET"
          - "PASSWORD"
          - "TOKEN"
          - "DATABASE_URL"
        condition: or

---

id: graphql-introspection

info:
  name: GraphQL Introspection Enabled
  author: dealbrief-scanner
  severity: medium
  description: Detects GraphQL endpoints with introspection enabled
  tags: graphql,introspection

http:
  - method: POST
    path:
      - "{{BaseURL}}/graphql"
      - "{{BaseURL}}/api/graphql"
      - "{{BaseURL}}/v1/graphql"
    
    body: |
      {"query": "{ __schema { types { name } } }"}
    
    headers:
      Content-Type: "application/json"

    matchers:
      - type: word
        words:
          - "__schema"
          - "types"
        condition: and

---

id: cors-misconfiguration

info:
  name: CORS Misconfiguration
  author: dealbrief-scanner
  severity: medium
  description: Detects CORS misconfigurations
  tags: cors,misconfiguration

http:
  - method: GET
    path:
      - "{{BaseURL}}"
    headers:
      Origin: "https://evil.com"

    matchers:
      - type: word
        part: header
        words:
          - "Access-Control-Allow-Origin: *"
          - "Access-Control-Allow-Origin: https://evil.com"
        condition: or
</file>

<file path="templates/testssl.conf">
# testssl.sh configuration
# Basic configuration for SSL/TLS testing

# Enable warnings
WARNINGS=true

# Set timeout
TIMEOUT=30

# Enable color output
COLOR=false

# Log level
QUIET=true
</file>

<file path="package.json">
{
  "name": "@dealbrief/workers",
  "version": "0.0.1",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "tsx watch worker.ts",
    "build": "tsc",
    "start": "node dist/worker.js"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.33.1",
    "@aws-sdk/client-s3": "^3.826.0",
    "@aws-sdk/s3-request-presigner": "^3.826.0",
    "@upstash/redis": "^1.34.3",
    "acorn": "^8.12.1",
    "aws-sdk": "^2.1691.0",
    "axios": "^1.7.2",
    "dotenv": "^16.4.7",
    "fast-xml-parser": "^4.4.0",
    "file-type": "^19.0.0",
    "langdetect": "^0.2.1",
    "luhn": "^2.4.1",
    "nanoid": "^5.0.9",
    "node-fetch": "^3.3.2",
    "node-html-parser": "^6.1.13",
    "openai": "^4.77.3",
    "pdfjs-dist": "^4.0.379",
    "pg": "^8.13.1",
    "puppeteer": "^23.11.1",
    "semver": "^7.6.3"
  },
  "devDependencies": {
    "@types/node": "^22.10.2",
    "@types/pg": "^8.11.10",
    "tsx": "^4.19.2",
    "typescript": "^5.7.2"
  }
}
</file>

<file path="tsconfig.json">
{
  "extends": "../../tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist"
  },
  "include": ["./**/*.ts"],
  "exclude": ["node_modules", "dist"]
}
</file>

<file path="validate-spiderfoot.ts">
#!/usr/bin/env tsx

import { execFile } from 'node:child_process';
import { promisify } from 'node:util';
import fs from 'node:fs/promises';
import { Pool } from 'pg';

const exec = promisify(execFile);

// Database connection
const pool = new Pool({
  connectionString: process.env.DATABASE_URL
});

let testResults = {
  artifacts: 0,
  subdomains: 0,
  ips: 0,
  emails: 0,
  intel: 0
};

async function log(message: string) {
  console.log(`[SpiderFoot-Validator] ${message}`);
}

async function runSpiderFootDirect(domain: string, scanId: string) {
  log(`Running SpiderFoot directly for ${domain} with scanId: ${scanId}`);
  
  try {
    // Clear any existing test artifacts
    await pool.query(`DELETE FROM artifacts WHERE meta->>'scan_id' = $1`, [scanId]);
    
    const timeout = parseInt(process.env.SPIDERFOOT_TIMEOUT_MS || '480000');
    log(`Using timeout: ${timeout}ms`);
    
    // Run SpiderFoot with JSON output
    const result = await exec('timeout', [
      `${Math.floor(timeout / 1000)}s`,
      'spiderfoot.py',
      '-s', domain,
      '-t', 'DOMAIN_NAME',
      '-m', 'sfp_dnsresolve,sfp_subdomain_enum,sfp_shodan,sfp_haveibeenpwned',
      '-o', 'json',
      '-q'
    ]);
    
    log(`SpiderFoot completed. Output size: ${result.stdout.length} bytes`);
    
    if (result.stdout.length < 50) {
      log(" SpiderFoot output is very small");
      return { artifactsCreated: 0, rawOutput: result.stdout, error: 'Minimal output' };
    }
    
    // Parse JSON output
    let jsonData;
    try {
      jsonData = JSON.parse(result.stdout);
    } catch (parseError) {
      log(` Failed to parse SpiderFoot JSON: ${parseError}`);
      return { artifactsCreated: 0, rawOutput: result.stdout, error: 'JSON parse failed' };
    }
    
    if (!Array.isArray(jsonData)) {
      log(" SpiderFoot output is not an array");
      return { artifactsCreated: 0, rawOutput: result.stdout, error: 'Invalid JSON structure' };
    }
    
    log(`SpiderFoot returned ${jsonData.length} raw results`);
    
    // Process results and create artifacts
    let artifactsCreated = 0;
    const keepAsIntel = new Set([
      'MALICIOUS_IPADDR', 'MALICIOUS_SUBDOMAIN', 'MALICIOUS_COHOST',
      'BLACKLISTED_IPADDR', 'BLACKLISTED_SUBDOMAIN', 'BLACKLISTED_COHOST',
      'VULNERABILITY', 'VULNERABILITY_CVE_CRITICAL', 'VULNERABILITY_CVE_HIGH',
      'BREACH_DATA', 'LEAKED_PASSWORD', 'DARKWEB_MENTION',
      'THREAT_INTEL', 'BOTNET_MEMBER', 'MALWARE_HASH'
    ]);
    
    for (const row of jsonData) {
      if (!row.type || !row.data) continue;
      
      let artifactType = 'intel';
      let value = row.data;
      
      // Categorize the finding
      if (row.type.includes('IP_ADDRESS') || row.type.includes('NETBLOCK')) {
        artifactType = 'ip';
      } else if (row.type.includes('SUBDOMAIN') || row.type.includes('DOMAIN')) {
        artifactType = 'subdomain';
      } else if (row.type.includes('EMAIL')) {
        artifactType = 'email';
      } else if (keepAsIntel.has(row.type)) {
        artifactType = 'threat';
      } else {
        continue; // Skip unknown types
      }
      
      // Insert artifact
      await pool.query(`
        INSERT INTO artifacts (type, val_text, meta, severity, created_at) 
        VALUES ($1, $2, $3, $4, NOW())
      `, [
        artifactType,
        value,
        JSON.stringify({
          scan_id: scanId,
          scan_module: 'spiderfoot',
          spiderfoot_type: row.type,
          confidence: row.confidence || 100,
          source_module: row.module || 'unknown'
        }),
        artifactType === 'threat' ? 'medium' : 'info'
      ]);
      
      artifactsCreated++;
      
      // Update test results
      switch (artifactType) {
        case 'subdomain':
          testResults.subdomains++;
          break;
        case 'ip':
          testResults.ips++;
          break;
        case 'email':
          testResults.emails++;
          break;
        case 'threat':
        case 'intel':
          testResults.intel++;
          break;
      }
    }
    
    // Create scan summary
    const summaryText = `SpiderFoot scan completed. Found ${artifactsCreated} artifacts from ${jsonData.length} raw results.`;
    await pool.query(`
      INSERT INTO artifacts (type, val_text, meta, severity, created_at) 
      VALUES ('scan_summary', $1, $2, $3, NOW())
    `, [
      summaryText,
      JSON.stringify({
        scan_id: scanId,
        scan_module: 'spiderfoot',
        artifacts_created: artifactsCreated,
        raw_results: jsonData.length,
        status: 'completed'
      }),
      'info'
    ]);
    
    testResults.artifacts = artifactsCreated;
    
    return {
      artifactsCreated,
      rawOutput: result.stdout,
      rawResults: jsonData.length,
      summary: summaryText
    };
    
  } catch (error: any) {
    log(`SpiderFoot test failed: ${error.message}`);
    
    // Create error summary
    const errorSummary = `SpiderFoot scan failed: ${error.message}`;
    await pool.query(`
      INSERT INTO artifacts (type, val_text, meta, severity, created_at) 
      VALUES ('scan_summary', $1, $2, $3, NOW())
    `, [
      errorSummary,
      JSON.stringify({
        scan_id: scanId,
        scan_module: 'spiderfoot',
        artifacts_created: 0,
        status: 'failed',
        error: error.message
      }),
      'error'
    ]);
    
    return {
      artifactsCreated: 0,
      rawOutput: '',
      error: error.message,
      summary: errorSummary
    };
  }
}

async function checkBinary() {
  log("Checking SpiderFoot binary availability...");
  
  try {
    // Always check if we need to create the wrapper script
    await fs.access('/opt/spiderfoot/sf.py');
    log("Found /opt/spiderfoot/sf.py, creating proper wrapper script...");
    
    // Create a wrapper script that runs SpiderFoot from its directory
    const wrapperScript = `#!/bin/bash
cd /opt/spiderfoot
python3 sf.py "$@"
`;
    await fs.writeFile('/usr/local/bin/spiderfoot.py', wrapperScript);
    await exec('chmod', ['+x', '/usr/local/bin/spiderfoot.py']);
    log(" Created SpiderFoot wrapper script");
    return true;
    
  } catch (error: any) {
    log(` Failed to create wrapper script: ${error.message}`);
    return false;
  }
}

async function checkApiKeys() {
  log("Checking API key availability...");
  
  const requiredKeys = [
    'SHODAN_API_KEY',
    'CENSYS_API_ID', 
    'CENSYS_API_SECRET',
    'HAVEIBEENPWNED_API_KEY'
  ];
  
  let missingKeys: string[] = [];
  let availableKeys: string[] = [];
  
  for (const key of requiredKeys) {
    if (process.env[key]) {
      availableKeys.push(` ${key}`);
    } else {
      missingKeys.push(` ${key}`);
    }
  }
  
  log(`API Keys Status:`);
  availableKeys.forEach(key => log(`  ${key}`));
  missingKeys.forEach(key => log(`  ${key}`));
  
  if (missingKeys.length > 0) {
    log(`Warning: ${missingKeys.length} API keys missing. SpiderFoot may have limited functionality.`);
  }
  
  return missingKeys.length === 0;
}

async function checkTimeout() {
  log("Checking timeout configuration...");
  
  const currentTimeout = process.env.SPIDERFOOT_TIMEOUT_MS || '300000';
  log(`Current timeout: ${currentTimeout}ms`);
  
  if (parseInt(currentTimeout) < 480000) {
    log("Increasing timeout to 8 minutes...");
    process.env.SPIDERFOOT_TIMEOUT_MS = '480000';
    return true;
  }
  
  return false;
}

async function analyzeSpiderFootOutput(scanId: string) {
  log("Analyzing SpiderFoot output types...");
  
  try {
    // Get all artifacts from the test run
    const result = await pool.query(`
      SELECT meta FROM artifacts 
      WHERE meta->>'scan_id' = $1 
      AND meta ? 'spiderfoot_type'
    `, [scanId]);
    
    const uniqueTypes = new Set<string>();
    
    for (const row of result.rows) {
      const spiderfootType = row.meta?.spiderfoot_type;
      if (spiderfootType) {
        uniqueTypes.add(spiderfootType);
      }
    }
    
    log(`Found SpiderFoot types: ${Array.from(uniqueTypes).join(', ')}`);
    
    // Save diagnostic info
    const diagnosticData = {
      timestamp: new Date().toISOString(),
      uniqueTypes: Array.from(uniqueTypes),
      totalArtifacts: result.rows.length,
      testResults
    };
    
    await fs.writeFile('/tmp/sf_diag.json', JSON.stringify(diagnosticData, null, 2));
    log("Diagnostic data saved to /tmp/sf_diag.json");
    
    return uniqueTypes.size > 0;
    
  } catch (error: any) {
    log(`Failed to analyze output: ${error.message}`);
    return false;
  }
}

async function main() {
  log("Starting SpiderFoot validation and self-healing...");
  
  let attempt = 1;
  const maxAttempts = 4;
  const testDomain = "example.com";
  const testScanId = "selftest";
  
  while (attempt <= maxAttempts) {
    log(`\n=== Attempt ${attempt}/${maxAttempts} ===`);
    
    // Step 1: Run SpiderFoot test
    const testResult = await runSpiderFootDirect(testDomain, testScanId);
    
    log(`Test result: ${testResult.artifactsCreated} artifacts created`);
    log(`Summary: ${testResult.summary || 'No summary'}`);
    log(`Raw output size: ${testResult.rawOutput?.length || 0} bytes`);
    
    // Step 2: Check if successful
    if (testResult.artifactsCreated > 0) {
      log(" SpiderFoot OK - artifacts created successfully!");
      
      // Step 4: Log final summary
      log("\n=== Final Summary ===");
      console.log(JSON.stringify(testResults, null, 2));
      
      process.exit(0);
    }
    
    // Step 3: Self-healing attempts
    log(` No artifacts created, attempting remediation...`);
    
    let remediated = false;
    
    // A. Binary check - always check if modules directory error occurs
    if (testResult.error?.includes('not found') || testResult.error?.includes('ENOENT') || testResult.error?.includes('Modules directory does not exist')) {
      log("Attempting binary remediation...");
      if (await checkBinary()) {
        remediated = true;
      }
    }
    
    // B. API key check (always check)
    if (!remediated) {
      log("Checking API keys...");
      await checkApiKeys(); // Always log status, but don't fail on missing keys
    }
    
    // C. Timeout check
    if (!remediated && (testResult.error?.includes('timeout') || (testResult.rawOutput?.length || 0) < 200)) {
      log("Attempting timeout remediation...");
      if (await checkTimeout()) {
        remediated = true;
      }
    }
    
    // D. Output analysis
    if (!remediated) {
      log("Analyzing SpiderFoot output types...");
      await analyzeSpiderFootOutput(testScanId);
    }
    
    if (!remediated && attempt === maxAttempts) {
      log(" All remediation attempts failed");
      
      // Final diagnostic output
      log("\n=== Diagnostic Information ===");
      log(`Final test results: ${JSON.stringify(testResults, null, 2)}`);
      log(`Last error: ${testResult.error || 'Unknown'}`);
      
      process.exit(1);
    }
    
    attempt++;
    
    // Wait before retry
    if (attempt <= maxAttempts) {
      log(`Waiting 5 seconds before retry...`);
      await new Promise(resolve => setTimeout(resolve, 5000));
    }
  }
}

// Handle cleanup
process.on('exit', async () => {
  try {
    await pool.end();
  } catch (error) {
    // Ignore cleanup errors
  }
});

main().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});
</file>

<file path="worker.ts">
import { config } from 'dotenv';
import { UpstashQueue } from './core/queue.js';
import { initializeDatabase, insertArtifact } from './core/artifactStore.js';
import { runShodanScan } from './modules/shodan.js';
import { runSpiderFoot } from './modules/spiderFoot.js';
import { runDocumentExposure } from './modules/documentExposure.js';
import { runTrufflehog } from './modules/trufflehog.js';
import { runRateLimitScan } from './modules/rateLimitScan.js';
import { runDnsTwist } from './modules/dnsTwist.js';
import { runTlsScan } from './modules/tlsScan.js';
import { runNuclei } from './modules/nuclei.js';
import { runDbPortScan } from './modules/dbPortScan.js';
import { runSpfDmarc } from './modules/spfDmarc.js';
import { runEndpointDiscovery } from './modules/endpointDiscovery.js';
import { pool } from './core/artifactStore.js';
import { supabase } from './core/supabaseClient.js';

config();

const queue = new UpstashQueue(process.env.REDIS_URL!);

function log(...args: any[]) {
  const timestamp = new Date().toISOString();
  console.log(`[${timestamp}] [worker]`, ...args);
}

interface ScanJob {
  id: string;
  companyName: string;
  domain: string;
  createdAt: string;
}

// All modules in execution order
const ALL_MODULES_IN_ORDER = [
  'spiderfoot',
  'dns_twist',
  'document_exposure',
  'shodan',
  'db_port_scan',
  'endpoint_discovery',
  'tls_scan',
  'nuclei',
  'rate_limit_scan',
  'spf_dmarc',
  'trufflehog'
];

interface ScanMasterUpdate {
  status?: string;
  progress?: number;
  current_module?: string;
  total_modules?: number;
  error_message?: string;
  total_findings_count?: number;
  max_severity?: string;
  completed_at?: Date;
}

// Helper function to update scans_master table
async function updateScanMasterStatus(scanId: string, updates: ScanMasterUpdate): Promise<void> {
  const setClause = Object.keys(updates)
    .map((key, index) => `${key} = $${index + 2}`)
    .join(', ');
  
  const values = [scanId, ...Object.values(updates)];
  
  await pool.query(
    `UPDATE scans_master SET ${setClause}, updated_at = NOW() WHERE scan_id = $1`,
    values
  );
  
  // Mirror to Supabase
  await supabase.from('scan_status').upsert({
    scan_id:         scanId,
    status:          updates.status        ?? undefined,
    progress:        updates.progress      ?? undefined,
    current_module:  updates.current_module?? undefined,
    error_message:   updates.error_message ?? undefined,
    max_severity:    updates.max_severity  ?? undefined,
    updated_at:      new Date().toISOString(),
  }, { onConflict: 'scan_id' }).throwOnError();
}

// Initialize scans_master table
async function initializeScansMasterTable(): Promise<void> {
  await pool.query(`
    CREATE TABLE IF NOT EXISTS scans_master (
      scan_id VARCHAR(255) PRIMARY KEY,
      company_name VARCHAR(255) NOT NULL,
      domain VARCHAR(255) NOT NULL,
      status VARCHAR(50) NOT NULL DEFAULT 'queued',
      progress INTEGER DEFAULT 0,
      current_module VARCHAR(100),
      total_modules INTEGER DEFAULT 0,
      error_message TEXT,
      total_findings_count INTEGER DEFAULT 0,
      max_severity VARCHAR(20),
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      completed_at TIMESTAMP WITH TIME ZONE
    );
    
    CREATE INDEX IF NOT EXISTS idx_scans_master_updated_at ON scans_master(updated_at);
    CREATE INDEX IF NOT EXISTS idx_scans_master_status ON scans_master(status);
  `);
}

async function processScan(job: ScanJob): Promise<void> {
  const { id: scanId, companyName, domain } = job;
  
  log(`Processing comprehensive security scan for ${companyName} (${domain})`);
  
  try {
    // === SCAN INITIALIZATION ===
    const TOTAL_MODULES = ALL_MODULES_IN_ORDER.length;
    
    // Insert or update scan record
    await pool.query(
      `INSERT INTO scans_master (scan_id, company_name, domain, status, progress, total_modules, created_at, updated_at)
       VALUES ($1, $2, $3, 'queued', 0, $4, NOW(), NOW())
       ON CONFLICT (scan_id) DO UPDATE SET 
         status = 'queued', 
         progress = 0,
         current_module = NULL,
         total_modules = $4,
         company_name = EXCLUDED.company_name,
         domain = EXCLUDED.domain,
         updated_at = NOW(),
         completed_at = NULL,
         error_message = NULL`,
      [scanId, companyName, domain, TOTAL_MODULES]
    );
    
    await queue.updateStatus(scanId, 'processing', 'Comprehensive security discovery in progress...');
    
    let totalFindings = 0;
    let modulesCompleted = 0;
    
    // === MODULE EXECUTION ===
    for (const moduleName of ALL_MODULES_IN_ORDER) {
      const progress = Math.floor((modulesCompleted / TOTAL_MODULES) * 100);
      
      // Update status before running module
      await updateScanMasterStatus(scanId, {
        status: 'processing',
        current_module: moduleName,
        progress: progress
      });
      
      log(`=== Running module: ${moduleName} (${modulesCompleted + 1}/${TOTAL_MODULES}) ===`);
      
      try {
        let moduleFindings = 0;
        
        switch (moduleName) {
          case 'spiderfoot':
            log(`Running SpiderFoot discovery for ${domain}`);
            moduleFindings = await runSpiderFoot({ domain, scanId });
            log(`SpiderFoot discovery completed: ${moduleFindings} targets found`);
            break;
            
          case 'dns_twist':
            log(`Running DNS Twist scan for ${domain}`);
            moduleFindings = await runDnsTwist({ domain, scanId });
            log(`DNS Twist completed: ${moduleFindings} typo-domains found`);
            break;
            
          case 'document_exposure':
            log(`Running document exposure scan for ${companyName}`);
            moduleFindings = await runDocumentExposure({ companyName, domain, scanId });
            log(`Document exposure completed: ${moduleFindings} discoveries`);
            break;
            
          case 'shodan':
            log(`Running Shodan scan for ${domain}`);
            console.log('[worker]  SHODAN SCAN STARTING');
            
            const apiKey = process.env.SHODAN_API_KEY;
            if (!apiKey) {
              throw new Error('SHODAN_API_KEY not configured');
            }
            
            const startTime = Date.now();
            moduleFindings = await runShodanScan({ domain, scanId, companyName });
            const duration = Date.now() - startTime;
            
            console.log('[worker]  SHODAN SCAN COMPLETED');
            console.log('[worker] Duration:', duration, 'ms');
            console.log('[worker] Findings:', moduleFindings);
            log(`Shodan infrastructure scan completed: ${moduleFindings} services found`);
            break;
            
          case 'db_port_scan':
            log(`Running database port scan for ${domain}`);
            moduleFindings = await runDbPortScan({ domain, scanId });
            log(`Database scan completed: ${moduleFindings} database issues found`);
            break;
            
          case 'endpoint_discovery':
            log(`Running endpoint discovery for ${domain}`);
            moduleFindings = await runEndpointDiscovery({ domain, scanId });
            log(`Endpoint discovery completed: ${moduleFindings} endpoint collections found`);
            break;
            
          case 'tls_scan':
            log(`Running TLS security scan for ${domain}`);
            moduleFindings = await runTlsScan({ domain, scanId });
            log(`TLS scan completed: ${moduleFindings} TLS issues found`);
            break;
            
          case 'nuclei':
            log(`Running Nuclei vulnerability scan for ${domain}`);
            moduleFindings = await runNuclei({ domain, scanId });
            log(`Nuclei scan completed: ${moduleFindings} vulnerabilities found`);
            break;
            
          case 'rate_limit_scan':
            log(`Running rate-limit tests for ${domain}`);
            moduleFindings = await runRateLimitScan({ domain, scanId });
            log(`Rate limiting tests completed: ${moduleFindings} rate limit issues found`);
            break;
            
          case 'spf_dmarc':
            log(`Running SPF/DMARC email security scan for ${domain}`);
            moduleFindings = await runSpfDmarc({ domain, scanId });
            log(`Email security scan completed: ${moduleFindings} email issues found`);
            break;
            
          case 'trufflehog':
            log(`Running TruffleHog secret detection for ${domain}`);
            moduleFindings = await runTrufflehog({ domain, scanId });
            log(`Secret detection completed: ${moduleFindings} secrets found`);
            break;
            
          default:
            log(`Unknown module: ${moduleName}, skipping`);
            break;
        }
        
        totalFindings += moduleFindings;
        modulesCompleted++;
        
        // Update progress after successful module completion
        const newProgress = Math.floor((modulesCompleted / TOTAL_MODULES) * 100);
        await updateScanMasterStatus(scanId, {
          progress: newProgress
        });
        
      } catch (moduleError) {
        log(`Module ${moduleName} failed:`, (moduleError as Error).message);
        
        // Update status to indicate module failure but continue
        await updateScanMasterStatus(scanId, {
          status: 'module_failed',
          error_message: `Module ${moduleName} failed: ${(moduleError as Error).message}`
        });
        
        // For critical modules, fail the entire scan
        if (moduleName === 'shodan' || moduleName === 'spiderfoot') {
          throw new Error(`Critical module ${moduleName} failed: ${(moduleError as Error).message}`);
        }
        
        // For non-critical modules, continue
        modulesCompleted++;
        const newProgress = Math.floor((modulesCompleted / TOTAL_MODULES) * 100);
        await updateScanMasterStatus(scanId, {
          status: 'processing', // Reset to processing after module failure
          progress: newProgress
        });
      }
    }

    // If no real findings, the scan failed
    if (totalFindings === 0) {
      throw new Error(`No real security findings discovered for ${domain}. Comprehensive scan failed to produce actionable results.`);
    }

    // === SCAN COMPLETION ===
    // Calculate findings stats
    const findingsStats = await pool.query(
        `SELECT 
            COUNT(*) as total_findings,
            MAX(CASE 
                WHEN severity = 'CRITICAL' THEN 5
                WHEN severity = 'HIGH' THEN 4
                WHEN severity = 'MEDIUM' THEN 3
                WHEN severity = 'LOW' THEN 2
                WHEN severity = 'INFO' THEN 1
                ELSE 0 
            END) as max_severity_score
         FROM findings WHERE scan_id = $1`,
        [scanId]
    );

    const totalFindingsCount = parseInt(findingsStats.rows[0]?.total_findings || '0');
    const maxSeverityScore = parseInt(findingsStats.rows[0]?.max_severity_score || '0');
    let maxSeverity = 'INFO';
    if (maxSeverityScore === 5) maxSeverity = 'CRITICAL';
    else if (maxSeverityScore === 4) maxSeverity = 'HIGH';
    else if (maxSeverityScore === 3) maxSeverity = 'MEDIUM';
    else if (maxSeverityScore === 2) maxSeverity = 'LOW';

    await updateScanMasterStatus(scanId, {
      status: 'done',
      progress: 100,
      completed_at: new Date(),
      total_findings_count: totalFindingsCount,
      max_severity: maxSeverity
    });

    await queue.updateStatus(
      scanId, 
      'done', 
      `Comprehensive security scan completed - ${totalFindings} verified findings across ${TOTAL_MODULES} security modules. Findings ready for processing.`
    );
    
    log(` COMPREHENSIVE SCAN COMPLETED for ${companyName}: ${totalFindings} verified findings across ${TOTAL_MODULES} security modules`);

  } catch (error) {
    log(` Scan failed for ${companyName}:`, (error as Error).message);
    
    // === SCAN FAILURE ===
    await updateScanMasterStatus(scanId, {
      status: 'failed',
      completed_at: new Date(),
      error_message: (error as Error).message
    });
    
    await queue.updateStatus(
      scanId, 
      'failed', 
      `Scan failed: ${(error as Error).message}`
    );
    
    // Store error artifact
    await insertArtifact({
      type: 'scan_error',
      val_text: `Comprehensive scan failed: ${(error as Error).message}`,
      severity: 'INFO',
      meta: {
        scan_id: scanId,
        company: companyName,
        error: true,
        timestamp: new Date().toISOString()
      }
    });
    
    throw error;
  }
}

async function startWorker() {
  log('Starting security scanning worker');
  
  // Validate required environment
  if (!process.env.SHODAN_API_KEY) {
    log('ERROR: SHODAN_API_KEY not configured - cannot run real scans');
    process.exit(1);
  }
  
  // Initialize database
  try {
    await initializeDatabase();
    await initializeScansMasterTable();
    log('Database and scans_master table initialized successfully');
  } catch (error) {
    log('Database initialization failed:', (error as Error).message);
    process.exit(1);
  }

  // Main processing loop
  while (true) {
    try {
      const job = await queue.getNextJob();
      
      if (job) {
        log('Processing scan job:', job.id);
        await processScan(job);
      } else {
        // No jobs available, wait
        await new Promise(resolve => setTimeout(resolve, 5000));
      }
      
    } catch (error) {
      log('Worker error:', (error as Error).message);
      await new Promise(resolve => setTimeout(resolve, 10000));
    }
  }
}

// Graceful shutdown
process.on('SIGTERM', () => {
  log('Received SIGTERM, shutting down...');
  process.exit(0);
});

process.on('SIGINT', () => {
  log('Received SIGINT, shutting down...');
  process.exit(0);
});

startWorker().catch(error => {
  log('CRITICAL: Failed to start worker:', (error as Error).message);
  process.exit(1);
});
</file>

</files>
